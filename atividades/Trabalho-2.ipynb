{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d19e28",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc6884",
   "metadata": {},
   "source": [
    "## Instalando as dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882f1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q matplotlib\n",
    "%pip install -q numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c016e3ee",
   "metadata": {},
   "source": [
    "## Importando o numpy e o matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7931d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829ca061",
   "metadata": {},
   "source": [
    "## Criando as funções básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf0697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_yPoints_linear_function(x: np.ndarray, bias: float, slope: float) -> np.ndarray:\n",
    "    \"\"\"Calcule the current y from f(x) = weight * slope + bias\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray | float): Current x in X axle\n",
    "        bias (float): The 'b' in f(x) = ax + b\n",
    "        slope (float): The 'a' in f(x) = ax + b\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: y value from the linear function\n",
    "    \"\"\"\n",
    "    return bias + (slope * x)\n",
    "\n",
    "def calcule_square_error(real_dots: np.ndarray[tuple], bias: float, slope: float) -> float:\n",
    "    \"\"\"Calcule the current square error\n",
    "\n",
    "    Args:\n",
    "        real_dots (np.ndarray[tuple]): Real points (x, y) collected out of IA\n",
    "        bias (float): The 'b' in f(x) = ax + b\n",
    "        slope (float): The 'a' in f(x) = ax + b\n",
    "\n",
    "    Returns:\n",
    "        float: calcule from square error\n",
    "    \"\"\"\n",
    "\n",
    "    # x_real and y_real are real values collecteds\n",
    "    # calcule the error of the linear regression in x_real\n",
    "    x_real: np.ndarray = real_dots[:, 0]\n",
    "    y_real: np.ndarray = real_dots[:, 1]\n",
    "\n",
    "    # y_pred is the IA response\n",
    "    y_pred: np.ndarray = calcule_yPoints_linear_function(x=x_real, bias=bias, slope=slope)\n",
    "\n",
    "    error_array: np.ndarray = y_real - y_pred\n",
    "\n",
    "    # all error array\n",
    "    return np.sum(error_array ** 2)\n",
    "\n",
    "def calcule_error_curve_values(x_ticks: np.ndarray, slope: float, real_dots: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Get all square error curve values in x linespace\n",
    "\n",
    "    Args:\n",
    "        x_ticks (np.ndarray): All x values to calculate (x axle)\n",
    "        slope (float): The 'a' in f(x) = ax + b (weight)\n",
    "        real_dots (np.ndarray): Real points (x, y) collected out of IA\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: All square error calculated\n",
    "    \"\"\"\n",
    "\n",
    "    # x_ticks are all avaliable bias\n",
    "    # bias = intecept\n",
    "    y_errors_values: list[float] = [calcule_square_error(slope=slope, bias=bias, real_dots=real_dots) for bias in x_ticks]\n",
    "\n",
    "    return np.array(y_errors_values)\n",
    "\n",
    "def calcule_partial_derivative_bias(y_real: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Get the partial derivative of square error curve\n",
    "\n",
    "    Args:\n",
    "        y_real (np.ndarray): Real points collected\n",
    "        y_pred (np.ndarray): Linear Regression points (same lenght of points with `y_real`)\n",
    "\n",
    "    Returns:\n",
    "        float: Partial derivative\n",
    "    \"\"\"\n",
    "\n",
    "    # -2 * (real_value - (bias + slope * x) + ... # x is a independent variable\n",
    "    # this is the patial derivate\n",
    "    # y_pred is a array with all values predicted by the linear regression\n",
    "    # y_real is a arrrary with all real values\n",
    "    return -2 * np.sum(y_real - y_pred)\n",
    "\n",
    "\n",
    "def calcule_tan_line(square_error: float, y_real: np.ndarray, y_pred: np.ndarray, bias: float, tan_range: float = 1.5) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Calcule the current tangent line of the square error curve\n",
    "\n",
    "    Args:\n",
    "        square_error (float): Error of the linear regression\n",
    "        y_real (np.ndarray): Real y values\n",
    "        y_pred (np.ndarray): Prediced y values by linear regression\n",
    "        bias (float): X in square error curve or (bias of the linear regression)\n",
    "        tan_range (float, optional): Range to calculate the tangent (x size). Defaults to 1.5.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]: Tuple with current x tangent values and y tangent values\n",
    "    \"\"\"\n",
    "\n",
    "    # bias is a x variable in square error curve\n",
    "    x_tan: np.ndarray = np.array([bias - tan_range, bias, bias + tan_range])\n",
    "\n",
    "    # equation of the straight line\n",
    "    # y = m * (x - x0) + y0\n",
    "    y_tan: np.ndarray = calcule_partial_derivative_bias(y_real, y_pred) * (x_tan - bias) + square_error\n",
    "    return (x_tan, y_tan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbbd1cb",
   "metadata": {},
   "source": [
    "## Funções para criar os gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933157c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "def create_subplots() -> tuple[Figure, tuple[Axes, Axes]]:\n",
    "    \"\"\"Create subplots to show animations\n",
    "\n",
    "    Returns:\n",
    "        tuple[Figure, tuple[Axes, ...]]: The figure and a tuple with all lines\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "    return fig, axes\n",
    "\n",
    "def config_axels(\n",
    "        axes: tuple[Axes, Axes], \n",
    "        real_dots: np.ndarray,\n",
    "        bias: float,\n",
    "        slope: float,\n",
    "        x_ticks_linear_regression: np.ndarray,\n",
    "        ypoints_linear_regression: np.ndarray,\n",
    "        x_ticks_error_curve: np.ndarray,\n",
    "        ypoints_square_error_curve: np.ndarray,\n",
    "        ) -> tuple:\n",
    "\n",
    "    # linear regrassion\n",
    "    ax1: Axes = axes[0]\n",
    "\n",
    "    # square error\n",
    "    ax2: Axes = axes[1]\n",
    "\n",
    "    linear_regression, = ax1.plot(x_ticks_linear_regression, ypoints_linear_regression)\n",
    "\n",
    "    # add individual real points\n",
    "    ax1.scatter(x=real_dots[:, 0], y=real_dots[:, 1], label='Pontos Reais', color='green', s=15, marker=\"o\")\n",
    "\n",
    "    ax1.set_title(\"Linear Regression\")\n",
    "    ax1.set_xlabel(\"weight\")\n",
    "    ax1.set_ylabel(\"height\")\n",
    "\n",
    "    # define origin to (0, 0)\n",
    "    ax1.set_xlim(left=0, right=x_ticks_linear_regression[-1])\n",
    "    ax1.set_ylim(bottom=0)\n",
    "\n",
    "    ax1.grid(visible=True, axis='both', alpha=0.5, color='gray')\n",
    "    bias_text = ax1.text(\n",
    "        0.1, # 10% x\n",
    "        0.9, # 90% y\n",
    "        f'Bias (intercept): {bias:.4f}',\n",
    "        transform=ax1.transAxes, \n",
    "        fontsize=12, \n",
    "        bbox=dict(facecolor='white', alpha=0.7)\n",
    "    )\n",
    "\n",
    "    ax2.plot(x_ticks_error_curve, ypoints_square_error_curve, label='Square Error')\n",
    "    ax2.set_title(\"Square Error\")\n",
    "    ax2.set_xlabel(\"Intercept (bias)\")\n",
    "    ax2.set_ylabel(\"Sum of Square Errors\")\n",
    "\n",
    "    square_error: float = calcule_square_error(real_dots, bias, 0.64)\n",
    "    x_real =  real_dots[:, 0]\n",
    "    y_real = real_dots[:, 1]\n",
    "    square_error_curve_tangent, = ax2.plot(\n",
    "        *calcule_tan_line(\n",
    "            square_error,\n",
    "            y_real,\n",
    "            calcule_yPoints_linear_function(x_real, bias, slope),\n",
    "            bias\n",
    "        ),\n",
    "        label='Derivative'\n",
    "        )\n",
    "    \n",
    "    ax2.grid(visible=True, axis='both', alpha=0.5, color='gray')\n",
    "    error_text = ax2.text(\n",
    "        0.1,\n",
    "        0.9,\n",
    "        f'Square Error: {square_error:.4f}',\n",
    "        transform=ax2.transAxes,\n",
    "        fontsize=12,\n",
    "        bbox=dict(facecolor='white', alpha=0.7)\n",
    "    )\n",
    "    \n",
    "    current_error_point, = ax2.plot(\n",
    "        [bias],\n",
    "        [square_error],\n",
    "        'ro',\n",
    "        markersize=8,\n",
    "        label='Current Error'\n",
    "    )\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    # define origin to (0, 0)\n",
    "    ax2.set_xlim(left=0, right=x_ticks_error_curve[-1])\n",
    "    ax2.set_ylim(bottom=0)\n",
    "\n",
    "    return linear_regression, square_error_curve_tangent, current_error_point, bias_text, error_text\n",
    "\n",
    "def gradient_descent_generator(\n",
    "        initial_bias: float,\n",
    "        slope: float,\n",
    "        real_dots: np.ndarray,\n",
    "        learn_rate: float,\n",
    "        max_iterations: int,\n",
    "    ) -> Generator[float]:\n",
    "    \"\"\"Generate a gradient descent to update the linear regression bias\n",
    "\n",
    "    Args:\n",
    "        initial_bias (float): First bias of the linear regression\n",
    "        slope (float): The 'a' of linear equation: f(x) = ax + b\n",
    "        real_dots (np.ndarray): Real dots of data\n",
    "        learn_rate (float): Multiply factor of the de step size\n",
    "        max_iterations (int): Limit of iterations\n",
    "\n",
    "    Yields:\n",
    "        Generator[float]: Generator with updated bias\n",
    "    \"\"\"\n",
    "\n",
    "    current_bias: float = initial_bias\n",
    "    for _ in range(max_iterations):\n",
    "        x_real: np.ndarray = real_dots[:, 0]\n",
    "        y_real: np.ndarray = real_dots[:, 1]\n",
    "        y_pred: np.ndarray = calcule_yPoints_linear_function(\n",
    "            x=x_real,\n",
    "            bias=current_bias,\n",
    "            slope=slope,\n",
    "        )\n",
    "\n",
    "        partial_derivative: float = calcule_partial_derivative_bias(\n",
    "            y_real=y_real,\n",
    "            y_pred=y_pred,\n",
    "        )\n",
    "\n",
    "        step_size: float = partial_derivative * learn_rate\n",
    "\n",
    "        current_bias -= step_size\n",
    "       \n",
    "        if step_size == 0:\n",
    "            break\n",
    "\n",
    "        yield current_bias\n",
    "        \n",
    "\n",
    "def animate( \n",
    "        current_bias: float,\n",
    "        x_ticks_linear_regression: np.ndarray,\n",
    "        real_dots: np.ndarray,\n",
    "        lines: tuple[Line2D, ...],\n",
    "        slope: float = 0.0,\n",
    "        tan_range: float = 1.5,\n",
    "        ) -> tuple[Line2D, ...]:\n",
    "    \"\"\"Animate the new values of the linear regression and the square error function\n",
    "\n",
    "    Args:\n",
    "        current_bias (float): Current 'b' (intercept) of the linear regression: f(x) = ax + b\n",
    "        x_ticks_linear_regression (np.ndarray): X values to calculate the linear regression\n",
    "        real_dots (np.ndarray): Real dots to use to adapt the linear regression\n",
    "        lines (tuple[Line2D, ...]): Lines with graphics\n",
    "        slope (float, optional): The 'a' (weight) of the linear regression: f(x) = ax + b. Defaults to 0.0.\n",
    "        tan_range (float, optional): Range of the tangent line. Defaults to 1.5.\n",
    "\n",
    "    Returns:\n",
    "        tuple[Line2D, ...]: All lines to update graphics\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_regression, square_error_curve_tan, current_error_point, bias_text, error_text = lines\n",
    "\n",
    "    y_points_linear_regression: np.ndarray = calcule_yPoints_linear_function(\n",
    "        x=x_ticks_linear_regression,\n",
    "        bias=current_bias,\n",
    "        slope=slope\n",
    "    )\n",
    "    linear_regression.set_data(x_ticks_linear_regression, y_points_linear_regression)\n",
    "    bias_text.set_text(f'Bias (intercept): {current_bias:.4f}')\n",
    "\n",
    "    y_real: np.ndarray = real_dots[:, 1]\n",
    "    x_real: np.ndarray = real_dots[:, 0]\n",
    "    y_pred: np.ndarray = calcule_yPoints_linear_function(\n",
    "        x=x_real, \n",
    "        bias=current_bias, \n",
    "        slope=slope\n",
    "    )\n",
    "\n",
    "    square_error: float = calcule_square_error(real_dots, current_bias, slope)\n",
    "    x_tan, y_tan = calcule_tan_line(\n",
    "        square_error=square_error,\n",
    "        y_real=y_real,\n",
    "        y_pred=y_pred,\n",
    "        bias=current_bias,\n",
    "        tan_range=tan_range\n",
    "    )\n",
    "    square_error_curve_tan.set_data(x_tan, y_tan)\n",
    "\n",
    "    # actual curve point\n",
    "    current_error_point.set_data([current_bias], [square_error])\n",
    "\n",
    "    error_text.set_text(f'Square Error: {square_error:.4f}')\n",
    "\n",
    "    return linear_regression, square_error_curve_tan, current_error_point, bias_text, error_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7832dd00",
   "metadata": {},
   "source": [
    "## Constantes para controlar a regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee5fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TICKS_LINEAR_REGRESSION: np.ndarray = np.linspace(0, 3.5)\n",
    "X_TICKS_ERROR_CURVE: np.ndarray = np.linspace(0, 2)\n",
    "REAL_DOTS: np.ndarray = np.array([[0.5, 1.4], [2.3, 1.9], [2.9, 3.2]])\n",
    "INITIAL_BIAS: int = 0\n",
    "SLOPE = 0.64\n",
    "LEARN_RATE: float = 0.1\n",
    "MAX_ITERATIONS: int = 150\n",
    "TAN_RANGE: float = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa52ffc",
   "metadata": {},
   "source": [
    "## Pegando os valores iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc788f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_points_linear_regression: np.ndarray = calcule_yPoints_linear_function(\n",
    "    x=X_TICKS_LINEAR_REGRESSION,\n",
    "    bias=INITIAL_BIAS,\n",
    "    slope=SLOPE,\n",
    ")\n",
    "\n",
    "y_points_square_error: np.ndarray = calcule_error_curve_values(\n",
    "    x_ticks=X_TICKS_ERROR_CURVE,\n",
    "    slope=SLOPE,\n",
    "    real_dots=REAL_DOTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de543dcb",
   "metadata": {},
   "source": [
    "## Criando a figura e os eixos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a50be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = create_subplots()\n",
    "lines_to_update = config_axels(\n",
    "    axes=axes,\n",
    "    bias=INITIAL_BIAS,\n",
    "    slope=SLOPE,\n",
    "    real_dots=REAL_DOTS,\n",
    "    x_ticks_linear_regression=X_TICKS_LINEAR_REGRESSION,\n",
    "    ypoints_linear_regression=y_points_linear_regression,\n",
    "    x_ticks_error_curve=X_TICKS_ERROR_CURVE,\n",
    "    ypoints_square_error_curve=y_points_square_error,\n",
    ")\n",
    "\n",
    "bias_generator = gradient_descent_generator(\n",
    "    initial_bias=INITIAL_BIAS,\n",
    "    slope=SLOPE,\n",
    "    real_dots=REAL_DOTS,\n",
    "    learn_rate=LEARN_RATE,\n",
    "    max_iterations=MAX_ITERATIONS\n",
    ")\n",
    "\n",
    "bias_frame_array: np.ndarray = np.array(list(bias_generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93f85c0",
   "metadata": {},
   "source": [
    "## Iniciando a animação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083cbff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "\n",
    "ani = FuncAnimation(\n",
    "    fig=fig,\n",
    "    func=animate,\n",
    "    frames=bias_frame_array,\n",
    "    fargs=(\n",
    "        X_TICKS_LINEAR_REGRESSION,\n",
    "        REAL_DOTS,\n",
    "        lines_to_update,\n",
    "        SLOPE,\n",
    "        TAN_RANGE,\n",
    "    ),\n",
    "    interval=100, # Interval between frames\n",
    "    blit=True,    # Otmization: just draw the changes\n",
    "    repeat=False  # Do not reapeat the animation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8104d8c",
   "metadata": {},
   "source": [
    "## Mostrando a animação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3599346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "plt.close(fig)\n",
    "HTML(ani.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
