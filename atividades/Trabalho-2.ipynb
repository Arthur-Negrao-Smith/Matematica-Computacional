{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d19e28",
   "metadata": {},
   "source": [
    "# Questão A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc6884",
   "metadata": {},
   "source": [
    "## Instalando as dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882f1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q matplotlib\n",
    "%pip install -q numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c016e3ee",
   "metadata": {},
   "source": [
    "## Importando o numpy e o matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7931d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "from typing import Any, Callable, Iterable, Literal\n",
    "\n",
    "from random import randint\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829ca061",
   "metadata": {},
   "source": [
    "## Criando as funções básicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee6f0d",
   "metadata": {},
   "source": [
    "### Função linear para calcular os valores dos pontos\n",
    "\n",
    "`x` será o conjunto de pontos que deseja ser calculado. `x` será um numpy array para permitir um \n",
    "cálculo vetorial mais eficiente.\n",
    "\n",
    "A ideia de vetorizar se dá a partir dos seguintes passos:\n",
    "\n",
    "A função linear se dá pela fórmula $f(x) = ax + b$, substituindo o $f(x)$ por $y$, temos: $y = ax + b$. \n",
    "Faremos isso para todo o conjunto de pontos, então podemos escrever:\n",
    "\n",
    "$\\{y_1, y_2, ..., y_n\\} = \\{ax_1 + b, ax_2 + b, \\cdots, ax_n + b\\}$\n",
    "\n",
    "Agora podemos transformar o conjunto de $y$ como um vetor $\\vec{y}$ de n dimensões:\n",
    "\n",
    "$\\vec{y} = \\{ax_1 + b, ax_2 + b, \\cdots, ax_n + b\\}$\n",
    "\n",
    "Outra forma de visualização de $\\vec{y}$ é por meio matricial:\n",
    "\n",
    "$$\n",
    "\\vec{y} = \n",
    "\\begin{pmatrix}\n",
    "y_1    \\\\\n",
    "y_2    \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Podemos isolar o $b$ que se repete em todos as colunas como um vetor $\\vec{b}$ de $n$ dimensões:\n",
    "\n",
    "$\\vec{y} = \\{ax_1, ax_2, \\cdots, ax_n\\} + \\vec{b}$\n",
    "\n",
    "Ou, na forma matricial:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "y_1    \\\\\n",
    "y_2    \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{pmatrix} =\n",
    "\n",
    "\\begin{pmatrix}\n",
    "ax_1   \\\\\n",
    "ax_2   \\\\\n",
    "\\vdots \\\\\n",
    "ax_n\n",
    "\\end{pmatrix}\n",
    "\n",
    "+\n",
    "\n",
    "\\begin{pmatrix}\n",
    "b      \\\\\n",
    "b      \\\\\n",
    "\\vdots \\\\\n",
    "b\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Agora, transformamos \"$a$\" em um escalar por estar multiplicando igualmente todos os $x$ e o \n",
    "conjunto $\\{x_1, x_2, ..., x_n\\}$ \n",
    "como um vetor $\\vec{x}$. Assim a fórmula pode ser reescrita por:\n",
    "\n",
    "$\\vec{y} = a\\vec{x} + \\vec{b}$\n",
    "\n",
    "Ou na forma matricial:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "y_1    \\\\\n",
    "y_2    \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{pmatrix} = a \\cdot\n",
    "\n",
    "\\begin{pmatrix}\n",
    "x_1    \\\\\n",
    "x_2    \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{pmatrix}\n",
    "\n",
    "+\n",
    "\n",
    "\\begin{pmatrix}\n",
    "b      \\\\\n",
    "b      \\\\\n",
    "\\vdots \\\\\n",
    "b\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Logo, é possível utilizar um array como um vetor para o cálculo inteiro, sendo o $\\vec{x}$ o vetor de\n",
    "entrada, o '$a$' representa o \"slope\" e o $\\vec{b}$ representa o \"bias\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef407e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_yPoints_linear_function(x: np.ndarray, bias: float, slope: float) -> np.ndarray:\n",
    "    \"\"\"Calcule the current y from f(x) = weight * slope + bias\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Current x in X axle\n",
    "        bias (float): The 'b' in f(x) = ax + b\n",
    "        slope (float): The 'a' in f(x) = ax + b\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: y values from the linear function\n",
    "    \"\"\"\n",
    "    return bias + (slope * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d9bc1",
   "metadata": {},
   "source": [
    "### Fórmula do erro quadrado para identificar a distância do previsto para a resposta\n",
    "\n",
    "Essa função está encarregada de pegar os pontos que temos para treinar o modelo e utiliza\n",
    "para calcular os valores de que a regressão linear está prevendo, utilizamos cálculos vetoriais\n",
    "novamente para otimizar o código python. Essa utilização do vetor surge a partir dos seguintes\n",
    "passos:\n",
    "\n",
    "$SE = \\sum_{i=1}^{n} (y_i - y'_i)^2$\n",
    "\n",
    "O \"$SE$\" representa o erro quadrado (_square error_), o $y_i$ representa o conjunto $Y$ de pontos pra \n",
    "treino e $y'_i$ é o conjunto $Y$ previsto pela regressão linear utilizando os mesmos valores de $x$ que os \n",
    "valores reais. Assim, podemos quebrar o somatório em um cálculo vetorial por $Y$ ser representado \n",
    "como um vetor assim como visto anteriormente:\n",
    "\n",
    "$e = (\\vec{y} - \\vec{y'})$\n",
    "\n",
    "$SE = \\sum e^2$\n",
    "\n",
    "**Obs:** Para fins de esclarecimento, informo que a jogada algébrica utilizada foi apenas de considerar o somatório\n",
    "dos $y_i$ e os $y'_i$ como vetores $\\vec{y}$ e $\\vec{y'}$ sendo subtraídos:\n",
    "\n",
    "$$\n",
    "e =\n",
    "\n",
    "\\begin{pmatrix}\n",
    "y_1 - y'_1 \\\\\n",
    "y_2 - y'_2 \\\\\n",
    "\\vdots     \\\\\n",
    "y_n - y'_n\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "e o $e^2$ seria o quadrado de cada subtração:\n",
    "\n",
    "$$\n",
    "e^2 =\n",
    "\n",
    "\\begin{pmatrix}\n",
    "(y_1 - y'_1)^2 \\\\\n",
    "(y_2 - y'_2)^2 \\\\\n",
    "\\vdots         \\\\\n",
    "(y_n - y'_n)^2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "E o $\\sum e^2$ representa a soma de todos os quadrados da diferença do vetor equivalente ao `np.sum(error_array ** 2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6038300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_square_error(real_dots: np.ndarray, bias: float, slope: float) -> float:\n",
    "    \"\"\"Calcule the current square error\n",
    "\n",
    "    Args:\n",
    "        real_dots (np.ndarray): Real points (x, y) collected out of IA\n",
    "        bias (float): The 'b' in f(x) = ax + b\n",
    "        slope (float): The 'a' in f(x) = ax + b\n",
    "\n",
    "    Returns:\n",
    "        float: calcule from square error\n",
    "    \"\"\"\n",
    "\n",
    "    # x_real and y_real are real values collecteds\n",
    "    # calcule the error of the linear regression in x_real\n",
    "    x_real: np.ndarray = real_dots[:, 0]\n",
    "    y_real: np.ndarray = real_dots[:, 1]\n",
    "\n",
    "    # y_pred is the IA response\n",
    "    y_pred: np.ndarray = calcule_yPoints_linear_function(x=x_real, bias=bias, slope=slope)\n",
    "\n",
    "    error_array: np.ndarray = y_real - y_pred\n",
    "\n",
    "    # sum all squareerror array\n",
    "    return np.sum(error_array ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79669c6b",
   "metadata": {},
   "source": [
    "## Função para calcular a curva do erro quadrado para os valores $x$ possíveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d11754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_error_curve_values(x_ticks: np.ndarray, slope: float, real_dots: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Get all square error curve values in x linespace\n",
    "\n",
    "    Args:\n",
    "        x_ticks (np.ndarray): All x values to calculate (x axle)\n",
    "        slope (float): The 'a' in f(x) = ax + b (weight)\n",
    "        real_dots (np.ndarray): Real points (x, y) collected out of IA\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: All square error calculated\n",
    "    \"\"\"\n",
    "    \n",
    "    # x_ticks are all avaliable bias\n",
    "    # bias = intecept\n",
    "    y_errors_values: list[float] = [calcule_square_error(slope=slope, bias=bias, real_dots=real_dots) for bias in x_ticks]\n",
    "\n",
    "    return np.array(y_errors_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1396382",
   "metadata": {},
   "source": [
    "## Função para calcular a derivada parcial da função do erro quadrado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4302301e",
   "metadata": {},
   "source": [
    "Essa função busca utilizar o numpy reduzindo o overhead padrão do python, o cálculo\n",
    "da derivada parcial se dá através da regra da cadeia como será demostrado adiante.\n",
    "\n",
    "A função do erro quadrado é definida como:\n",
    "\n",
    "$SE = \\sum (\\vec{y} - \\vec{y'})^2$\n",
    "\n",
    "Sendo $\\vec{y'}$ uma abreviação que pode ser reescrita como:\n",
    "\n",
    "$\\vec{y'} = (a \\cdot \\vec{x} + \\vec{b})$\n",
    "\n",
    "Porém, podemos representar a subtração vetorial como:\n",
    "\n",
    "$e = (\\vec{y} - \\vec{y'})$\n",
    "\n",
    "Agora podemos derivar o valor $SE$ em relação a \"$b$\": \n",
    "\n",
    "$\\frac{\\partial SE}{\\partial \\vec{b}} = \\frac{\\partial SE}{\\partial \\vec{y'}}\\sum (\\vec{y} - (a \\cdot \\vec{x} + \\vec{b}))^2$\n",
    "\n",
    "Podemos mudar utilizar \"$e$\" no lugar de $(\\vec{y} - (a \\cdot \\vec{x} + \\vec{b}))$ e utilizar a regra da cadeia:\n",
    "\n",
    "$\\frac{\\partial SE}{\\partial \\vec{b}} = \\sum \\frac{\\partial}{\\partial e}e^2 \\frac{\\partial}{\\partial \\vec{b}}(\\vec{y} - \\vec{y'})$\n",
    "\n",
    "A derivada de $SE$ em relação a \"$e$\":\n",
    "\n",
    "1. $\\frac{\\partial SE}{\\partial e} = \\frac{\\partial}{\\partial b}e^2$\n",
    "2. $\\frac{\\partial SE}{\\partial e} = 2e$\n",
    "\n",
    "A derivada de $e$ em relação a $\\vec{y}$:\n",
    "\n",
    "1. $\\frac{\\partial e}{\\partial \\vec{b}} = \\frac{\\partial}{\\partial \\vec{b}}(\\vec{y} - (a \\cdot \\vec{x} + \\vec{b}))$\n",
    "2. $\\frac{\\partial e}{\\partial \\vec{b}} = -1$\n",
    "\n",
    "A derivada final:\n",
    "\n",
    "$\\frac{\\partial SE}{\\partial \\vec{b}} = \\sum 2e \\times (-1)$\n",
    "\n",
    "Podemos novamente substituir o valor de \"$e$\" por $(\\vec{y} - \\vec{y'})$ e passar as constantes multiplicativas para fora do somatório\n",
    "e obtemos o valor da derivada parcial:\n",
    "\n",
    "$\\frac{\\partial SE}{\\partial b} = -2 \\sum (\\vec{y} - \\vec{y'})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf0697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_partial_derivative_bias(y_real: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Get the partial derivative of square error curve\n",
    "\n",
    "    Args:\n",
    "        y_real (np.ndarray): Real points collected\n",
    "        y_pred (np.ndarray): Linear Regression points (same lenght of points with `y_real`)\n",
    "\n",
    "    Returns:\n",
    "        float: Partial derivative\n",
    "    \"\"\"\n",
    "\n",
    "    # -2 * (real_value - (bias + slope * x) + ... # x is a independent variable\n",
    "    # this is the patial derivate\n",
    "    # y_pred is a array with all values predicted by the linear regression\n",
    "    # y_real is a arrrary with all real values\n",
    "    return -2 * np.sum(y_real - y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c383a1",
   "metadata": {},
   "source": [
    "## Função para calcular a tangente do erro quadrado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b53ed",
   "metadata": {},
   "source": [
    "Essa função usará a equação da reta para gerar uma reta tangente com a fim de\n",
    "demonstrar a derivada em relação àquele ponto. Como o \"$m$\" representa a inclinação\n",
    "da reta, então ele será equivalente a derivada parcial do _bias_.\n",
    "\n",
    "A equação da reta possui a seguinte fórmula: $y = m * (x - x_0) + y_0$\n",
    "\n",
    "Como o \"$y$\", o \"$y_0$\" o \"$x$\" e o \"$x_0$\" representam um conjunto de pontos, posso novamente representa-los\n",
    "como vetores, respectivamente: $\\vec{y}$, $\\vec{y_0}$, $\\vec{x}$ e $\\vec{x_0}$. Matricialmente essa fórmula\n",
    "fica:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "y_1    \\\\\n",
    "y_2    \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{pmatrix} = m \\cdot\n",
    "\n",
    "\\left[\n",
    "\\begin{pmatrix}\n",
    "x_1    \\\\\n",
    "x_2    \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{pmatrix} - \\vec{x_0}\n",
    "\\right]\n",
    "\n",
    "+ \\vec{y_0}\n",
    "$$\n",
    "\n",
    "Simplificando a notação $\\vec{x}-{x_0}$:\n",
    "\n",
    "$$\n",
    "\\vec{x} - \\vec{x_0} = \n",
    "\n",
    "\\begin{pmatrix}\n",
    "x_1 - x_0 \\\\\n",
    "x_2 - x_0 \\\\\n",
    "\\vdots    \\\\\n",
    "x_n - x_0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Agora podemos representar o cálculo como:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "y_1    \\\\\n",
    "y_2    \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{pmatrix} = m \\cdot\n",
    "\n",
    "\\begin{pmatrix}\n",
    "x_1 - x_0 \\\\\n",
    "x_2 - x_0 \\\\\n",
    "\\vdots    \\\\\n",
    "x_n - x_0\n",
    "\\end{pmatrix} +\n",
    "\n",
    "\\begin{pmatrix}\n",
    "y_0    \\\\\n",
    "y_0    \\\\\n",
    "\\vdots \\\\\n",
    "y_0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**Obs:** Lembrando que o $x_0$ e o $y_0$ representam, respectivamente, o $b$ (`bias`) e o erro quadrado (`square_error`) daquele ponto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f073c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_tan_line(square_error: float, y_real: np.ndarray, y_pred: np.ndarray, bias: float, tan_range: float = 1.5) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Calcule the current tangent line of the square error curve\n",
    "\n",
    "    Args:\n",
    "        square_error (float): Error of the linear regression\n",
    "        y_real (np.ndarray): Real y values\n",
    "        y_pred (np.ndarray): Prediced y values by linear regression\n",
    "        bias (float): X in square error curve or (bias of the linear regression)\n",
    "        tan_range (float, optional): Range to calculate the tangent (x size). Defaults to 1.5.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]: Tuple with current x tangent values and y tangent values\n",
    "    \"\"\"\n",
    "\n",
    "    # bias is a x variable in square error curve\n",
    "    x_tan: np.ndarray = np.linspace(\n",
    "        start=bias - tan_range,\n",
    "        stop=bias + tan_range,\n",
    "        num=len(y_pred)\n",
    "    )\n",
    "\n",
    "    # equation of the straight line\n",
    "    # y = m * (x - x0) + y0\n",
    "    y_tan: np.ndarray = calcule_partial_derivative_bias(y_real, y_pred) * (x_tan - bias) + square_error\n",
    "    return (x_tan, y_tan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f98b3d7",
   "metadata": {},
   "source": [
    "## Criando uma classe para ser o Animator das animações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8450c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animator:\n",
    "    \"\"\"\n",
    "    Class to create animations with matplotlib.\n",
    "\n",
    "    This class abstracts the creation of the Figure, the initial configuration \n",
    "        of artists (lines, text, etc.), and the animation lifecycle, simplifying \n",
    "        display in interactive environments like Jupyter Notebooks.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        create_plots_function: Callable[[], tuple[Figure, tuple[Axes, ...]]], \n",
    "        init_function: Callable[[Any], tuple[Line2D, ...]],\n",
    "        update_function: Callable[[Any], tuple[Line2D, ...]],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        \"\"\"Constructor from `Animator`\n",
    "\n",
    "        Args:\n",
    "            create_plots_function (Callable[[], tuple[Figure, tuple[Axes, ...]]]): Function \n",
    "                that creates the Matplotlib Figure and Axes. It must return a tuple containing \n",
    "                the Figure and a tuple of the created Axes.\n",
    "\n",
    "            init_function (Callable[[Any], tuple[Line2D, ...]]): Function that receives \n",
    "                the Axes and extra arguments (`*args`, `**kwargs`) to set up the initial \n",
    "                visual state. It must return a tuple with all the artist objects\n",
    "                (`Line2D`, `Text`, etc.) that need to be updated during the animation.\n",
    "\n",
    "\n",
    "            update_function (Callable[[Any], tuple[Line2D, ...]]): Function that updates \n",
    "                the graphical data (the animation logic). The first parameter receives \n",
    "                the frame data (the value from `frames`), and subsequent parameters \n",
    "                receive arguments passed via `fargs` of the `plot()` method.\n",
    "                This function MUST return a tuple with all modified artists.\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If the `create_plots_function` does not return a `Figure` \n",
    "                object, or if there is an error creating the Matplotlib animation.\n",
    "\n",
    "        Returns:\n",
    "            None: The constructor returns nothing.\n",
    "        \"\"\"\n",
    "        self._create_plots: Callable = create_plots_function\n",
    "        self._init_func: Callable = init_function\n",
    "        self._update_func: Callable = update_function\n",
    "        \n",
    "        self._fig, self._axes = self._create_plots()\n",
    "\n",
    "        if not isinstance(self._fig, Figure):\n",
    "            raise RuntimeError(f\"The configure function don't return a matplot Figure: {type(self._fig)}\")\n",
    "        \n",
    "        self._lines: tuple = self._init_func(self._axes, *args, **kwargs)\n",
    "        \n",
    "        self._ani: FuncAnimation | None = None\n",
    "\n",
    "    def _create_animation(self, frames: Iterable, fargs: tuple, *args, **kwargs) -> None:\n",
    "        new_fargs: tuple = (self._lines,) + fargs\n",
    "\n",
    "        self._ani = FuncAnimation(\n",
    "            fig=self._fig,\n",
    "            fargs=new_fargs,\n",
    "            func=self._update_func,\n",
    "            frames=frames, \n",
    "            interval=kwargs.pop(\"interval\", 100), # Interval between frames\n",
    "            blit=kwargs.pop(\"blit\", True),        # Otmization: just draw the changes\n",
    "            repeat=kwargs.pop(\"repeat\", False),   # Do not reapeat the animation\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "    def plot(self, frames: Iterable, fargs: tuple, *args, **kwargs) -> None:\n",
    "        self._create_animation(frames, fargs, *args, **kwargs)\n",
    "\n",
    "        if self._ani is None:\n",
    "            raise RuntimeError(f\"Erro to create the matplotlib animation.\")\n",
    "        \n",
    "        plt.close(self._fig)\n",
    "        display(HTML(self._ani.to_jshtml()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbbd1cb",
   "metadata": {},
   "source": [
    "## Funções para criar os gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933157c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subplots() -> tuple[Figure, tuple[Axes, Axes]]:\n",
    "    \"\"\"Create subplots to show animations\n",
    "\n",
    "    Returns:\n",
    "        tuple[Figure, tuple[Axes, ...]]: The figure and a tuple with all lines\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "    return fig, axes\n",
    "\n",
    "def config_axels(\n",
    "        axes: tuple[Axes, Axes], \n",
    "        real_dots: np.ndarray,\n",
    "        bias: float,\n",
    "        slope: float,\n",
    "        x_ticks_linear_regression: np.ndarray,\n",
    "        ypoints_linear_regression: np.ndarray,\n",
    "        x_ticks_error_curve: np.ndarray,\n",
    "        ypoints_square_error_curve: np.ndarray,\n",
    "        ) -> tuple:\n",
    "\n",
    "    # linear regrassion\n",
    "    ax1: Axes = axes[0]\n",
    "\n",
    "    # square error\n",
    "    ax2: Axes = axes[1]\n",
    "\n",
    "    linear_regression, = ax1.plot(x_ticks_linear_regression, ypoints_linear_regression)\n",
    "\n",
    "    # add individual real points\n",
    "    ax1.scatter(x=real_dots[:, 0], y=real_dots[:, 1], label='Pontos Reais', color='green', s=15, marker=\"o\")\n",
    "\n",
    "    ax1.set_title(\"Linear Regression\")\n",
    "    ax1.set_xlabel(\"weight\")\n",
    "    ax1.set_ylabel(\"height\")\n",
    "\n",
    "    # define origin to (0, 0)\n",
    "    ax1.set_xlim(left=0, right=x_ticks_linear_regression[-1])\n",
    "    ax1.set_ylim(bottom=0)\n",
    "\n",
    "    ax1.grid(visible=True, axis='both', alpha=0.5, color='gray')\n",
    "    bias_text = ax1.text(\n",
    "        0.1, # 10% x\n",
    "        0.9, # 90% y\n",
    "        f'Bias (intercept): {bias:.4f}',\n",
    "        transform=ax1.transAxes, \n",
    "        fontsize=12, \n",
    "        bbox=dict(facecolor='white', alpha=0.7)\n",
    "    )\n",
    "\n",
    "    ax2.plot(x_ticks_error_curve, ypoints_square_error_curve, label='Square Error')\n",
    "    ax2.set_title(\"Square Error\")\n",
    "    ax2.set_xlabel(\"Intercept (bias)\")\n",
    "    ax2.set_ylabel(\"Sum of Square Errors\")\n",
    "\n",
    "    square_error: float = calcule_square_error(real_dots, bias, 0.64)\n",
    "    x_real =  real_dots[:, 0]\n",
    "    y_real = real_dots[:, 1]\n",
    "    square_error_curve_tangent, = ax2.plot(\n",
    "        *calcule_tan_line(\n",
    "            square_error,\n",
    "            y_real,\n",
    "            calcule_yPoints_linear_function(x_real, bias, slope),\n",
    "            bias\n",
    "        ),\n",
    "        label='Derivative'\n",
    "        )\n",
    "    \n",
    "    ax2.grid(visible=True, axis='both', alpha=0.5, color='gray')\n",
    "    error_text = ax2.text(\n",
    "        0.1,\n",
    "        0.9,\n",
    "        f'Square Error: {square_error:.4f}',\n",
    "        transform=ax2.transAxes,\n",
    "        fontsize=12,\n",
    "        bbox=dict(facecolor='white', alpha=0.7)\n",
    "    )\n",
    "    \n",
    "    current_error_point, = ax2.plot(\n",
    "        [bias],\n",
    "        [square_error],\n",
    "        'ro',\n",
    "        markersize=8,\n",
    "        label='Current Error'\n",
    "    )\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    # define origin to (0, 0)\n",
    "    ax2.set_xlim(left=0, right=x_ticks_error_curve[-1])\n",
    "    ax2.set_ylim(bottom=0)\n",
    "\n",
    "    return linear_regression, square_error_curve_tangent, current_error_point, bias_text, error_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61f7c3b",
   "metadata": {},
   "source": [
    "## Criando uma função para calcular o gradiente descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cff23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_array(\n",
    "        initial_bias: float,\n",
    "        slope: float,\n",
    "        real_dots: np.ndarray,\n",
    "        learn_rate: float,\n",
    "        max_iterations: int,\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"Create a gradient descent array to update the linear regression bias\n",
    "\n",
    "    Args:\n",
    "        initial_bias (float): First bias of the linear regression\n",
    "        slope (float): The 'a' of linear equation: f(x) = ax + b\n",
    "        real_dots (np.ndarray): Real dots of data\n",
    "        learn_rate (float): Multiply factor of the de step size\n",
    "        max_iterations (int): Limit of iterations\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray[float]: Array with updated bias\n",
    "    \"\"\"\n",
    "\n",
    "    bias_list: list[float] = []\n",
    "    current_bias: float = initial_bias\n",
    "    for _ in range(max_iterations):\n",
    "        x_real: np.ndarray = real_dots[:, 0]\n",
    "        y_real: np.ndarray = real_dots[:, 1]\n",
    "        y_pred: np.ndarray = calcule_yPoints_linear_function(\n",
    "            x=x_real,\n",
    "            bias=current_bias,\n",
    "            slope=slope,\n",
    "        )\n",
    "\n",
    "        partial_derivative: float = calcule_partial_derivative_bias(\n",
    "            y_real=y_real,\n",
    "            y_pred=y_pred,\n",
    "        )\n",
    "\n",
    "        step_size: float = partial_derivative * learn_rate\n",
    "\n",
    "        current_bias -= step_size\n",
    "       \n",
    "        if step_size == 0:\n",
    "            break\n",
    "\n",
    "        bias_list.append(current_bias)\n",
    "    \n",
    "    return np.array(bias_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e7f51d",
   "metadata": {},
   "source": [
    "## Criando uma função para calcular as variações do gráfico para criar a animação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272585d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate( \n",
    "        current_bias: float,\n",
    "        lines: tuple[Line2D, ...],\n",
    "        x_ticks_linear_regression: np.ndarray,\n",
    "        real_dots: np.ndarray,\n",
    "        slope: float = 0.0,\n",
    "        tan_range: float = 1.5,\n",
    "        ) -> tuple[Line2D, ...]:\n",
    "    \"\"\"Animate the new values of the linear regression and the square error function\n",
    "\n",
    "    Args:\n",
    "        current_bias (float): Current 'b' (intercept) of the linear regression: f(x) = ax + b\n",
    "        lines (tuple[Line2D, ...]): Lines with graphics\n",
    "        x_ticks_linear_regression (np.ndarray): X values to calculate the linear regression\n",
    "        real_dots (np.ndarray): Real dots to use to adapt the linear regression\n",
    "        slope (float, optional): The 'a' (weight) of the linear regression: f(x) = ax + b. Defaults to 0.0.\n",
    "        tan_range (float, optional): Range of the tangent line. Defaults to 1.5.\n",
    "\n",
    "    Returns:\n",
    "        tuple[Line2D, ...]: All lines to update graphics\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_regression, square_error_curve_tan, current_error_point, bias_text, error_text = lines\n",
    "\n",
    "    y_points_linear_regression: np.ndarray = calcule_yPoints_linear_function(\n",
    "        x=x_ticks_linear_regression,\n",
    "        bias=current_bias,\n",
    "        slope=slope\n",
    "    )\n",
    "    linear_regression.set_data(x_ticks_linear_regression, y_points_linear_regression)\n",
    "    bias_text.set_text(f'Bias (intercept): {current_bias:.4f}')\n",
    "\n",
    "    y_real: np.ndarray = real_dots[:, 1]\n",
    "    x_real: np.ndarray = real_dots[:, 0]\n",
    "    y_pred: np.ndarray = calcule_yPoints_linear_function(\n",
    "        x=x_real, \n",
    "        bias=current_bias, \n",
    "        slope=slope\n",
    "    )\n",
    "\n",
    "    square_error: float = calcule_square_error(real_dots, current_bias, slope)\n",
    "    x_tan, y_tan = calcule_tan_line(\n",
    "        square_error=square_error,\n",
    "        y_real=y_real,\n",
    "        y_pred=y_pred,\n",
    "        bias=current_bias,\n",
    "        tan_range=tan_range\n",
    "    )\n",
    "    square_error_curve_tan.set_data(x_tan, y_tan)\n",
    "\n",
    "    # actual curve point\n",
    "    current_error_point.set_data([current_bias], [square_error])\n",
    "\n",
    "    error_text.set_text(f'Square Error: {square_error:.4f}')\n",
    "\n",
    "    return linear_regression, square_error_curve_tan, current_error_point, bias_text, error_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7832dd00",
   "metadata": {},
   "source": [
    "## Constantes para controlar a regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee5fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TICKS_LINEAR_REGRESSION: np.ndarray = np.linspace(0, 3.5)\n",
    "X_TICKS_ERROR_CURVE: np.ndarray = np.linspace(0, 2)\n",
    "REAL_DOTS: np.ndarray = np.array([[0.5, 1.4], [2.3, 1.9], [2.9, 3.2]])\n",
    "INITIAL_BIAS: float = 0.0\n",
    "SLOPE = 0.64\n",
    "LEARN_RATE: float = 0.1\n",
    "MAX_ITERATIONS: int = 150\n",
    "TAN_RANGE: float = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa52ffc",
   "metadata": {},
   "source": [
    "## Pegando os valores iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc788f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_points_linear_regression: np.ndarray = calcule_yPoints_linear_function(\n",
    "    x=X_TICKS_LINEAR_REGRESSION,\n",
    "    bias=INITIAL_BIAS,\n",
    "    slope=SLOPE,\n",
    ")\n",
    "\n",
    "y_points_square_error: np.ndarray = calcule_error_curve_values(\n",
    "    x_ticks=X_TICKS_ERROR_CURVE,\n",
    "    slope=SLOPE,\n",
    "    real_dots=REAL_DOTS,\n",
    ")\n",
    "\n",
    "bias_frame_array: np.ndarray = gradient_descent_array(\n",
    "    initial_bias=INITIAL_BIAS,\n",
    "    slope=SLOPE,\n",
    "    real_dots=REAL_DOTS,\n",
    "    learn_rate=LEARN_RATE,\n",
    "    max_iterations=MAX_ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e052186",
   "metadata": {},
   "source": [
    "## Iniciando o animador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f601a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "animator = Animator(\n",
    "    # Animator params\n",
    "    create_plots_function=create_subplots,\n",
    "    init_function=config_axels,\n",
    "    update_function=animate,\n",
    "\n",
    "    # Config function params\n",
    "    bias=INITIAL_BIAS,\n",
    "    slope=SLOPE,\n",
    "    real_dots=REAL_DOTS,\n",
    "    x_ticks_linear_regression=X_TICKS_LINEAR_REGRESSION,\n",
    "    ypoints_linear_regression=y_points_linear_regression,\n",
    "    x_ticks_error_curve=X_TICKS_ERROR_CURVE,\n",
    "    ypoints_square_error_curve=y_points_square_error,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50815439",
   "metadata": {},
   "source": [
    "## Mostrando a animação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889434e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "animator.plot(\n",
    "    frames=bias_frame_array,\n",
    "    fargs=(\n",
    "        X_TICKS_LINEAR_REGRESSION,\n",
    "        REAL_DOTS,\n",
    "        SLOPE,\n",
    "        TAN_RANGE,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18054971",
   "metadata": {},
   "source": [
    "# Questão B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab5db8",
   "metadata": {},
   "source": [
    "## Função para calcular a derivada do slope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e41dc2",
   "metadata": {},
   "source": [
    "Essa função busca utilizar o numpy reduzindo o overhead padrão do python, o cálculo\n",
    "da derivada parcial se dá através da regra da cadeia como será demostrado adiante.\n",
    "\n",
    "A função do erro quadrado é definida como:\n",
    "\n",
    "$SE = \\sum (\\vec{y} - \\vec{y'})^2$\n",
    "\n",
    "Sendo $\\vec{y'}$ uma abreviação que pode ser reescrita como:\n",
    "\n",
    "$\\vec{y'} = (a \\cdot \\vec{x} + \\vec{b})$\n",
    "\n",
    "Porém, podemos representar a subtração vetorial como:\n",
    "\n",
    "$e = (\\vec{y} - \\vec{y'})$\n",
    "\n",
    "Agora podemos derivar o valor $SE$ em relação a \"$a$\": \n",
    "\n",
    "$\\frac{\\partial SE}{\\partial a} = \\frac{\\partial SE}{\\partial \\vec{y'}}\\sum (\\vec{y} - (a \\cdot \\vec{x} + \\vec{b}))^2$\n",
    "\n",
    "Podemos mudar utilizar \"$e$\" no lugar de $(\\vec{y} - (a \\cdot \\vec{x} + \\vec{b}))$ e utilizar a regra da cadeia:\n",
    "\n",
    "$\\frac{\\partial SE}{\\partial a} = \\sum \\frac{\\partial}{\\partial e}e^2 \\frac{\\partial}{\\partial a}(\\vec{y} - \\vec{y'})$\n",
    "\n",
    "A derivada de $SE$ em relação a \"$e$\":\n",
    "\n",
    "1. $\\frac{\\partial SE}{\\partial e} = \\frac{\\partial}{\\partial b}e^2$\n",
    "2. $\\frac{\\partial SE}{\\partial e} = 2e$\n",
    "\n",
    "A derivada de $e$ em relação a \"$a$\":\n",
    "\n",
    "1. $\\frac{\\partial e}{\\partial a} = \\frac{\\partial}{\\partial a}(\\vec{y} - (a \\cdot \\vec{x} + \\vec{b}))$\n",
    "2. $\\frac{\\partial e}{\\partial a} = -1 \\cdot \\vec{x}$\n",
    "\n",
    "A derivada final:\n",
    "\n",
    "$\\frac{\\partial SE}{\\partial a} = \\sum 2e \\times (-1 \\cdot \\vec{x})$\n",
    "\n",
    "Podemos novamente substituir o valor de \"$e$\" por $(\\vec{y} - \\vec{y'})$ e passar as constantes multiplicativas para fora do somatório\n",
    "e obtemos o valor da derivada parcial:\n",
    "\n",
    "$\\frac{\\partial SE}{\\partial a} = -2 \\cdot \\vec{x} \\times \\sum (\\vec{y} - \\vec{y'})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c3d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_partial_derivative_slope(x_real: np.ndarray, y_real: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return -2 * np.sum(x_real*(y_real - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487873a",
   "metadata": {},
   "source": [
    "## Funções para criar as animações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b87f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bias_slope_subplots() -> tuple:\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "    return fig, axes\n",
    "\n",
    "def init_bias_slope_axle(\n",
    "    axes: tuple[Axes, ...],\n",
    "    real_dots: np.ndarray,\n",
    "    bias: float,\n",
    "    slope: float,\n",
    "    x_ticks_linear_regression: np.ndarray,\n",
    "    ):\n",
    "\n",
    "    ax1: Axes = axes[0]\n",
    "    ax2: Axes = axes[1]\n",
    "\n",
    "    y_linear_values: np.ndarray = calcule_yPoints_linear_function(\n",
    "        x=x_ticks_linear_regression,\n",
    "        bias=bias,\n",
    "        slope=slope\n",
    "    )\n",
    "\n",
    "    stochastic_linear_regression, = ax1.plot(\n",
    "        x_ticks_linear_regression,\n",
    "        y_linear_values,\n",
    "        label=\"Linear Regression\"\n",
    "    )\n",
    "\n",
    "    # add individual real points\n",
    "    ax1.scatter(x=real_dots[:, 0], y=real_dots[:, 1], label='Real Points', color='green', s=15, marker=\"o\")\n",
    "\n",
    "    ax1.set_title(\"Stochastic Linear Regression\")\n",
    "    ax1.set_xlabel(\"weight\")\n",
    "    ax1.set_ylabel(\"height\")\n",
    "\n",
    "    # define origin to (0, 0)\n",
    "    ax1.set_xlim(left=0, right=x_ticks_linear_regression[-1])\n",
    "    ax1.set_ylim(bottom=0)\n",
    "\n",
    "    ax1.grid(visible=True, axis='both', alpha=0.5, color='gray')\n",
    "    stochastic_bias_text = ax1.text(\n",
    "        0.1, # 10% x\n",
    "        0.9, # 90% y\n",
    "        f'Bias (intercept): {bias:.4f}',\n",
    "        transform=ax1.transAxes,\n",
    "        fontsize=12,\n",
    "        bbox=dict(facecolor='white', alpha=0.7)\n",
    "    )\n",
    "\n",
    "    stochastic_slope_text = ax1.text(\n",
    "        0.1, # 10% x\n",
    "        0.8, # 80% y\n",
    "        f'Slope: {slope:.4f}',\n",
    "        transform=ax1.transAxes,\n",
    "        fontsize=12,\n",
    "        bbox=dict(facecolor='white', alpha=0.7)\n",
    "    )\n",
    "\n",
    "    ax1.legend()\n",
    "\n",
    "    minibatch_linear_regression, = ax2.plot(\n",
    "        x_ticks_linear_regression,\n",
    "        y_linear_values,\n",
    "        label='Linear Regression'\n",
    "    )\n",
    "\n",
    "    ax2.scatter(x=real_dots[:, 0], y=real_dots[:, 1], label='Real Points', color='green', s=15, marker=\"o\")\n",
    "\n",
    "    ax2.set_title(\"Mini-Batch (2) Linear Regression\")\n",
    "    ax2.set_xlabel(\"weight\")\n",
    "    ax2.set_ylabel(\"height\")\n",
    "\n",
    "    # define origin to (0, 0)\n",
    "    ax2.set_xlim(left=0, right=x_ticks_linear_regression[-1])\n",
    "    ax2.set_ylim(bottom=0)\n",
    "    \n",
    "    ax2.grid(visible=True, axis='both', alpha=0.5, color='gray')\n",
    "    minibatch_bias_text = ax2.text(\n",
    "        0.1, # 10% x\n",
    "        0.9, # 90% y\n",
    "        f'Bias (intercept): {bias:.4f}',\n",
    "        transform=ax2.transAxes,\n",
    "        fontsize=12,\n",
    "        bbox=dict(facecolor='white', alpha=0.7)\n",
    "    )\n",
    "\n",
    "    minibatch_slope_text = ax2.text(\n",
    "        0.1, # 10% x\n",
    "        0.8, # 80% y\n",
    "        f'Slope: {slope:.4f}',\n",
    "        transform=ax2.transAxes,\n",
    "        fontsize=12,\n",
    "        bbox=dict(facecolor='white', alpha=0.7)\n",
    "    )\n",
    "\n",
    "    # define origin to (0, 0)\n",
    "    ax2.set_xlim(left=0, right=x_ticks_linear_regression[-1])\n",
    "    ax2.set_ylim(bottom=0)\n",
    "\n",
    "    ax2.legend()\n",
    "\n",
    "    return (stochastic_linear_regression, \n",
    "            stochastic_bias_text, \n",
    "            stochastic_slope_text, \n",
    "            minibatch_linear_regression, \n",
    "            minibatch_bias_text, \n",
    "            minibatch_slope_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3585d05",
   "metadata": {},
   "source": [
    "## Criando a função para o cálculo dos gradientes de cada tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e370e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiante_descent_array_bias_and_slope(\n",
    "    initial_bias: float,\n",
    "    initial_slope: float,\n",
    "    real_dots: np.ndarray,\n",
    "    learn_rate: float,\n",
    "    max_iterations: int,\n",
    ") -> np.ndarray:\n",
    "    \n",
    "    stochastic_bias_slope_list: list[list[float]] = []\n",
    "    stochastic_current_bias: float = initial_bias\n",
    "    stochastic_current_slope: float = initial_slope\n",
    "    for _ in range(max_iterations):\n",
    "        x_real: np.ndarray = real_dots[:, 0]\n",
    "        y_real: np.ndarray = real_dots[:, 1]\n",
    "        \n",
    "        stochastic_choice: int = randint(0, len(x_real)-1)\n",
    "        stochastic_x_real = np.array([x_real[stochastic_choice]])\n",
    "        stochastic_y_real = np.array([y_real[stochastic_choice]])\n",
    "        stochastic_y_pred: np.ndarray = calcule_yPoints_linear_function(\n",
    "            x=stochastic_x_real,\n",
    "            bias=stochastic_current_bias,\n",
    "            slope=stochastic_current_slope,\n",
    "        )\n",
    "\n",
    "        stochastic_bias_partial_derivative: float = calcule_partial_derivative_bias(\n",
    "            y_real=stochastic_y_real,\n",
    "            y_pred=stochastic_y_pred,\n",
    "        )\n",
    "\n",
    "        stochastic_slope_partial_derivative: float = calcule_partial_derivative_slope(\n",
    "            x_real=stochastic_x_real,\n",
    "            y_real=stochastic_y_real,\n",
    "            y_pred=stochastic_y_pred\n",
    "        )\n",
    "\n",
    "        stochastic_bias_step_size: float = stochastic_bias_partial_derivative * learn_rate\n",
    "        stochastic_slope_step_size: float = stochastic_slope_partial_derivative * learn_rate\n",
    "\n",
    "        if stochastic_bias_step_size != 0:\n",
    "            stochastic_current_bias -= stochastic_bias_step_size\n",
    "\n",
    "        if stochastic_slope_step_size != 0:\n",
    "            stochastic_current_slope -= stochastic_slope_step_size\n",
    "       \n",
    "        if stochastic_bias_step_size == 0 and stochastic_slope_step_size == 0:\n",
    "            break\n",
    "\n",
    "        stochastic_bias_slope_list.append([stochastic_current_bias, stochastic_current_slope])\n",
    "    \n",
    "    minibatch_current_bias: float = initial_bias\n",
    "    minibatch_current_slope: float = initial_slope\n",
    "    minibatch_bias_slope_list: list[list[float]] = []\n",
    "    for _ in range(max_iterations):\n",
    "        x_real: np.ndarray = real_dots[:, 0]\n",
    "        y_real: np.ndarray = real_dots[:, 1]\n",
    "        \n",
    "        minibatch_choice_1: int = randint(0, len(x_real)-1)\n",
    "        valid: bool = False\n",
    "        while not valid:\n",
    "            minibatch_choice_2 = randint(0, len(x_real)-1)\n",
    "            if minibatch_choice_2 != minibatch_choice_1:\n",
    "                valid = True\n",
    "\n",
    "        minibatch_x_real = np.array([x_real[minibatch_choice_1], x_real[minibatch_choice_2]])\n",
    "        minibatch_y_real = np.array([y_real[minibatch_choice_1], y_real[minibatch_choice_2]])\n",
    "        minibatch_y_pred: np.ndarray = calcule_yPoints_linear_function(\n",
    "            x=minibatch_x_real,\n",
    "            bias=minibatch_current_bias,\n",
    "            slope=minibatch_current_slope,\n",
    "        )\n",
    "\n",
    "        minibatch_bias_partial_derivative: float = calcule_partial_derivative_bias(\n",
    "            y_real=minibatch_y_real,\n",
    "            y_pred=minibatch_y_pred,\n",
    "        )\n",
    "\n",
    "        minibatch_slope_partial_derivative: float = calcule_partial_derivative_slope(\n",
    "            x_real=minibatch_x_real,\n",
    "            y_real=minibatch_y_real,\n",
    "            y_pred=minibatch_y_pred\n",
    "        )\n",
    "\n",
    "        minibatch_bias_step_size: float = minibatch_bias_partial_derivative * learn_rate\n",
    "        minibatch_slope_step_size: float = minibatch_slope_partial_derivative * learn_rate\n",
    "\n",
    "        if minibatch_bias_step_size != 0:\n",
    "            minibatch_current_bias -= minibatch_bias_step_size\n",
    "\n",
    "        if minibatch_slope_step_size != 0:\n",
    "            minibatch_current_slope -= minibatch_slope_step_size\n",
    "       \n",
    "        if minibatch_bias_step_size == 0 and minibatch_slope_step_size == 0:\n",
    "            break\n",
    "\n",
    "        minibatch_bias_slope_list.append([minibatch_current_bias, minibatch_current_slope])\n",
    "\n",
    "    aux_list: list = [stochastic_sublist+minibatch_sublist for stochastic_sublist, minibatch_sublist in zip(stochastic_bias_slope_list, minibatch_bias_slope_list)]\n",
    "\n",
    "    return np.array(aux_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e319be2",
   "metadata": {},
   "source": [
    "## Criando a função de atualização de ambos os parâmetros (bias e slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_bias_slope(\n",
    "    bias_slope: np.ndarray,\n",
    "    lines: tuple[Line2D, ...],\n",
    "    x_ticks_linear_regression: np.ndarray,\n",
    ") -> tuple:\n",
    "    (stochastic_linear_regression, \n",
    "        stochastic_bias_text, \n",
    "        stochastic_slope_text, \n",
    "        minibatch_linear_regression, \n",
    "        minibatch_bias_text, \n",
    "        minibatch_slope_text) = lines\n",
    "\n",
    "    stochastic_current_bias: float = bias_slope[0]\n",
    "    stochastic_current_slope: float = bias_slope[1]\n",
    "    minibatch_current_bias: float = bias_slope[2]\n",
    "    minibatch_current_slope: float = bias_slope[3]\n",
    "\n",
    "    stochastic_y_points_linear_regression: np.ndarray = calcule_yPoints_linear_function(\n",
    "        x=x_ticks_linear_regression,\n",
    "        bias=stochastic_current_bias,\n",
    "        slope=stochastic_current_slope\n",
    "    )\n",
    "    stochastic_linear_regression.set_data(x_ticks_linear_regression, stochastic_y_points_linear_regression)\n",
    "    stochastic_bias_text.set_text(f'Bias (intercept): {stochastic_current_bias:.4f}')\n",
    "    stochastic_slope_text.set_text(f'Slope: {stochastic_current_slope:.4f}')\n",
    "\n",
    "    minibatch_y_points_linear_regression: np.ndarray = calcule_yPoints_linear_function(\n",
    "        x=x_ticks_linear_regression,\n",
    "        bias=minibatch_current_bias,\n",
    "        slope=minibatch_current_slope\n",
    "    )\n",
    "    minibatch_linear_regression.set_data(x_ticks_linear_regression, minibatch_y_points_linear_regression)\n",
    "    minibatch_bias_text.set_text(f'Bias (intercept): {minibatch_current_bias:.4f}')\n",
    "    minibatch_slope_text.set_text(f'Slope: {minibatch_current_slope:.4f}')\n",
    "\n",
    "    return (stochastic_linear_regression, \n",
    "            stochastic_bias_text, \n",
    "            stochastic_slope_text, \n",
    "            minibatch_linear_regression, \n",
    "            minibatch_bias_text, \n",
    "            minibatch_slope_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f90add3",
   "metadata": {},
   "source": [
    "## Criando constantes para a simulação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b309fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TICKS_LINEAR_REGRESSION: np.ndarray = np.linspace(0, 3.5)\n",
    "REAL_DOTS: np.ndarray = np.array([[0.5, 1.4], [2.3, 1.9], [2.9, 3.2]])\n",
    "INITIAL_BIAS: float = 0.0\n",
    "INITIAL_SLOPE: float = 1.0\n",
    "LEARN_RATE: float = 0.05\n",
    "MAX_ITERATIONS: int = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69565783",
   "metadata": {},
   "source": [
    "## Criando o animador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfddf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "animator: Animator = Animator(\n",
    "    create_plots_function=create_bias_slope_subplots,\n",
    "    init_function=init_bias_slope_axle,\n",
    "    update_function=update_bias_slope,\n",
    "    bias=INITIAL_BIAS,\n",
    "    slope=INITIAL_SLOPE,\n",
    "    real_dots=REAL_DOTS,\n",
    "    x_ticks_linear_regression=X_TICKS_LINEAR_REGRESSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d5b30",
   "metadata": {},
   "source": [
    "## Iniciando o bias e slope array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9d621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_slope_frame_array: np.ndarray = gradiante_descent_array_bias_and_slope(\n",
    "    initial_bias=INITIAL_BIAS,\n",
    "    initial_slope=INITIAL_SLOPE,\n",
    "    real_dots=REAL_DOTS,\n",
    "    learn_rate=LEARN_RATE,\n",
    "    max_iterations=MAX_ITERATIONS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9ebfc1",
   "metadata": {},
   "source": [
    "## Exibindo a animação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a954a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "animator.plot(\n",
    "    frames=bias_slope_frame_array,\n",
    "    fargs=(\n",
    "        X_TICKS_LINEAR_REGRESSION,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1a0b75",
   "metadata": {},
   "source": [
    "# Questão D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc8d2ab",
   "metadata": {},
   "source": [
    "## Criando as funções de ativação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5105998",
   "metadata": {},
   "source": [
    "$\\sigma(x) = \\frac{1}{\n",
    "    1 + e^x\n",
    "}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66013ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    s: np.ndarray = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softplus(x: np.ndarray) -> np.ndarray:\n",
    "    return np.log(1 + np.exp(1)**x)\n",
    "\n",
    "def softplus_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    return sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8e2a4",
   "metadata": {},
   "source": [
    "## Criando uma classe para abstrair a camada de neurôrions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df35ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        output_size: int,\n",
    "        weights: np.ndarray | None = None,\n",
    "        biases: np.ndarray | None = None,\n",
    "    ) -> None:\n",
    "        \n",
    "        self.input_size: int = input_size\n",
    "        self.output_size: int = output_size\n",
    "\n",
    "        # weights to multiply\n",
    "        if weights is None:\n",
    "            self._weights: np.ndarray = np.random.randn(input_size, output_size) # 2D\n",
    "        else:\n",
    "            self._weights = weights\n",
    "\n",
    "        # biases to sum\n",
    "        if biases is None:\n",
    "            self._biases: np.ndarray = np.random.randn(output_size) # 1D\n",
    "        else:\n",
    "            self._biases = biases\n",
    "\n",
    "    @property\n",
    "    def weights(self) -> np.ndarray:\n",
    "        return self._weights\n",
    "    \n",
    "    @weights.setter\n",
    "    def weights(self, new_weights: np.ndarray) -> None:\n",
    "        self._weights = new_weights\n",
    "\n",
    "    @property\n",
    "    def biases(self) -> np.ndarray:\n",
    "        return self._biases\n",
    "\n",
    "    @biases.setter\n",
    "    def biases(self, new_biases: np.ndarray) -> None:\n",
    "        self._biases = new_biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dbc7cf",
   "metadata": {},
   "source": [
    "## Criando uma base abstrata para redes neurais das questões D e E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca3a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNeuralNetwork(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_layers: list[DenseLayer],\n",
    "        output_layer: DenseLayer,\n",
    "        activation_function: Literal[\"softplus\", \"sigmoid\"] = \"softplus\",\n",
    "    ) -> None:\n",
    "        self._input_size: int = input_size\n",
    "\n",
    "        if input_size != hidden_layers[0].input_size:\n",
    "            raise ValueError(\"Input size mismatch in the first hidden layer.\")\n",
    "    \n",
    "        self._hidden_layers: list[DenseLayer] = hidden_layers\n",
    "        self._output_layer: DenseLayer = output_layer\n",
    "\n",
    "        functions_dict: dict[str, Callable[[np.ndarray], np.ndarray]] = {\n",
    "            \"softplus\": softplus, \n",
    "            \"sigmoid\": sigmoid,\n",
    "        }\n",
    "        derivatives_dict: dict[str, Callable[[np.ndarray], np.ndarray]] = {\n",
    "            \"softplus\": softplus_derivative, \n",
    "            \"sigmoid\": sigmoid_derivative,\n",
    "        }\n",
    "\n",
    "        self._activation_function: Callable[[np.ndarray], np.ndarray] = functions_dict[activation_function]\n",
    "        self._derivative_function: Callable[[np.ndarray], np.ndarray] = derivatives_dict[activation_function]\n",
    "\n",
    "        self._cache_Z: list[np.ndarray] = []\n",
    "        self._cache_A: list[np.ndarray] = []\n",
    "\n",
    "    @property\n",
    "    def output_layer(self) -> DenseLayer:\n",
    "        return self._output_layer\n",
    "\n",
    "    @property\n",
    "    def hidden_layers(self) -> list[DenseLayer]:\n",
    "        return self._hidden_layers\n",
    "\n",
    "    def predict(self, input_array: np.ndarray, use_activate_func_in_output: bool = False) -> np.ndarray:\n",
    "        # to ensure that the array will have one dimension\n",
    "        if input_array.ndim == 1:\n",
    "            input_array = input_array.reshape(-1, 1)\n",
    "    \n",
    "        if input_array.shape[-1] != self._input_size:\n",
    "            raise ValueError(f\"The input layer must have the size of: {self._input_size}\")\n",
    "\n",
    "        current_output: np.ndarray = input_array\n",
    "\n",
    "        self._cache_Z.clear()\n",
    "        self._cache_A.clear()\n",
    "\n",
    "        for layer in self.hidden_layers:\n",
    "            weights_matrix: np.ndarray = layer.weights\n",
    "            biases_matrix: np.ndarray = layer.biases\n",
    "\n",
    "            # calcule output to cache\n",
    "            Z: np.ndarray = current_output @ weights_matrix + biases_matrix\n",
    "\n",
    "            current_output = self._activation_function(Z)\n",
    "\n",
    "            self._cache_Z.append(Z)\n",
    "            self._cache_A.append(current_output)\n",
    "\n",
    "        Z = current_output @ self.output_layer.weights + self.output_layer.biases\n",
    "\n",
    "        if use_activate_func_in_output:\n",
    "            current_output = self._activation_function(Z)\n",
    "        else:\n",
    "            current_output = Z\n",
    "\n",
    "        self._cache_Z.append(Z)\n",
    "        self._cache_A.append(current_output)\n",
    "\n",
    "        return current_output\n",
    "\n",
    "    @staticmethod\n",
    "    def _calcule_total_square_error(y_real: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        return np.sum((y_real - y_pred) ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _calcule_square_error_derivative_float(y_real: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"Get the partial derivative of square error\n",
    "\n",
    "        Args:\n",
    "            y_real (np.ndarray): Real points collected\n",
    "            y_pred (np.ndarray): Linear Regression points (same lenght of points with `y_real`)\n",
    "\n",
    "        Returns:\n",
    "            float: Partial derivative\n",
    "        \"\"\"\n",
    "\n",
    "        # -2 * (real_value - (bias + slope * x) + ... # x is a independent variable\n",
    "        # this is the patial derivate\n",
    "        # y_pred is a array with all values predicted by the neural network\n",
    "        # y_real is a arrary with all real values\n",
    "        return -2 * np.sum(y_real - y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calcule_square_error_derivative_array(y_real: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Get the partial derivative of square error vector\n",
    "\n",
    "        Args:\n",
    "            y_real (np.ndarray): Real points collected\n",
    "            y_pred (np.ndarray): Linear Regression points (same lenght of points with `y_real`)\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Partial derivative array\n",
    "        \"\"\"\n",
    "\n",
    "        # -2 * (real_value - (bias + slope * x) + ... # x is a independent variable\n",
    "        # this is the patial derivate\n",
    "        # y_pred is a array with all values predicted by the neural network\n",
    "        # y_real is a arrary with all real values\n",
    "        return -2 * (y_real - y_pred)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(\n",
    "        self,\n",
    "        x_train: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        learning_rate: float = 0.01,\n",
    "        epochs: int = 100,\n",
    "        tolerance: float = 10e-5,\n",
    "        ) -> None:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            x_train (np.ndarray): X axle to calculate the predict output\n",
    "            y_train (np.ndarray): Y axle to compare the predict output\n",
    "            learning_rate (float, optional): Rate of the derivative is apply. Defaults to 0.01.\n",
    "            epochs (int, optional): Number of iterations to train. Defaults to 100.\n",
    "            tolerance (float, optional): Number of tolerance to stop the training. Defaults to 5.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36b077f",
   "metadata": {},
   "source": [
    "## Criando uma classe específica para a questão D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9634a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork_D(BaseNeuralNetwork):\n",
    "    def __init__(\n",
    "        self, input_size: int, \n",
    "        hidden_layers: list[DenseLayer],\n",
    "        output_layer: DenseLayer,\n",
    "        activation_function: Literal['softplus'] | Literal['sigmoid'] = \"softplus\"\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            input_size, \n",
    "            hidden_layers, \n",
    "            output_layer, \n",
    "            activation_function\n",
    "        )\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        x_train: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        learning_rate: float = 0.01,\n",
    "        epochs: int = 100,\n",
    "        tolerance: float = 10e-5,\n",
    "    ) -> None:\n",
    "        \n",
    "        if x_train.ndim == 1: x_train = x_train.reshape(-1, 1)\n",
    "        if y_train.ndim == 1: y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            y_pred: np.ndarray = self.predict(x_train)\n",
    "\n",
    "            square_error_derivative: float = self._calcule_square_error_derivative_float(y_real=y_train, y_pred=y_pred)\n",
    "\n",
    "            step_size: float = square_error_derivative * learning_rate\n",
    "\n",
    "            if abs(step_size) <= tolerance:\n",
    "                break\n",
    "            \n",
    "            print(\"-\"*50)\n",
    "            print(f\"Old b_3: {self.output_layer.biases}\")\n",
    "\n",
    "            self.output_layer.biases = self.output_layer.biases - step_size\n",
    "\n",
    "            print(f\"Step size: {step_size}\")\n",
    "            print(f\"New b_3: {self.output_layer.biases}\")\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            print(\"-\"*50)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878b2794",
   "metadata": {},
   "source": [
    "## Criando e mostrando o gráfico gerado pela rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e51347",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [\n",
    "    DenseLayer(\n",
    "        input_size=1,\n",
    "        output_size=2,\n",
    "        weights=np.array([ \n",
    "            [3.34, -3.53]\n",
    "        ]),\n",
    "        biases=np.array(\n",
    "            [-1.43, 0.57]\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "output_layer: DenseLayer = DenseLayer(\n",
    "    input_size=2,\n",
    "    output_size=1,\n",
    "    weights=np.array([ \n",
    "        [-1.22], [-2.30]\n",
    "    ]),\n",
    "    biases=np.array(\n",
    "        #[2.61]\n",
    "        [0.0]\n",
    "    )\n",
    ")\n",
    "\n",
    "nn = NeuralNetwork_D(\n",
    "    input_size=1,\n",
    "    hidden_layers=hidden_layers,\n",
    "    output_layer=output_layer\n",
    ")\n",
    "\n",
    "x_ticks: np.ndarray = np.linspace(0, 1)\n",
    "y_pred: np.ndarray = nn.predict(x_ticks)\n",
    "\n",
    "plt.plot(x_ticks, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b7a42d",
   "metadata": {},
   "source": [
    "## Treinando e mostrando o gráfico com o valor de b_3 atualizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN: np.ndarray = np.array([0.0, 0.5, 1.0])\n",
    "Y_TRAIN: np.ndarray = np.array([0.0, 1.0, 0.0])\n",
    "nn.fit(\n",
    "    x_train=X_TRAIN,\n",
    "    y_train=Y_TRAIN\n",
    ")\n",
    "\n",
    "x_ticks: np.ndarray = np.linspace(0, 1)\n",
    "y_pred: np.ndarray = nn.predict(x_ticks)\n",
    "\n",
    "plt.plot(x_ticks, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762374c",
   "metadata": {},
   "source": [
    "# Questão E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17ef632",
   "metadata": {},
   "source": [
    "## Criando uma rede neural específica para a questão E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac62d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork_E(BaseNeuralNetwork):\n",
    "    def __init__(\n",
    "        self, input_size: int, \n",
    "        hidden_layers: list[DenseLayer],\n",
    "        output_layer: DenseLayer,\n",
    "        activation_function: Literal['softplus'] | Literal['sigmoid'] = \"softplus\"\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            input_size,\n",
    "            hidden_layers,\n",
    "            output_layer,\n",
    "            activation_function\n",
    "        )\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        x_train: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        learning_rate: float = 0.01,\n",
    "        epochs: int = 100,\n",
    "        tolerance: float = 10e-5,\n",
    "    ) -> None:\n",
    "       \n",
    "        if x_train.ndim == 1: x_train = x_train.reshape(-1, 1)\n",
    "        if y_train.ndim == 1: y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            y_pred: np.ndarray = self.predict(x_train)\n",
    "\n",
    "            output_gradient_vec: np.ndarray = self._calcule_square_error_derivative_array(y_real=y_train, y_pred=y_pred)\n",
    "\n",
    "            output_delta: np.ndarray = output_gradient_vec\n",
    "\n",
    "            # if the input of the layer is a output from the last hidden layer\n",
    "            if len(self._cache_A) > 1:\n",
    "                input_to_output: np.ndarray = self._cache_A[-2]\n",
    "            else:\n",
    "                raise RuntimeError(\"No hidden layers in neural network.\")\n",
    "            \n",
    "            gradient_w_output: np.ndarray = input_to_output.T @ output_delta\n",
    "            gradient_b_output: np.ndarray = np.sum(output_delta, axis=0)\n",
    "\n",
    "            output_layer_weights_step_size: np.ndarray = learning_rate * gradient_w_output\n",
    "            output_layer_bias_step_size: np.ndarray = learning_rate * gradient_b_output\n",
    "            \n",
    "            if (\n",
    "                abs(output_layer_bias_step_size[0]) <= tolerance and \n",
    "                abs(output_layer_weights_step_size[0, 0]) <= tolerance and \n",
    "                abs(output_layer_weights_step_size[1, 0]) <= tolerance\n",
    "            ):\n",
    "                break\n",
    "\n",
    "            print(\"=\"*50)\n",
    "            print(f\"Old b3: {self.output_layer.biases[0]}\")\n",
    "            print(f\"b3 step size: {output_layer_bias_step_size[0]}\")\n",
    "            print(\"-\"*25)\n",
    "            print(f\"Old w3: {self.output_layer.weights[0, 0]}\")\n",
    "            print(f\"w3 step size: {output_layer_weights_step_size[0, 0]}\")\n",
    "            print(\"-\"*25)\n",
    "            print(f\"Old w4: {self.output_layer.weights[1, 0]}\")\n",
    "            print(f\"w4 step size: {output_layer_weights_step_size[1, 0]}\")\n",
    "            print(\"-\"*25)\n",
    "\n",
    "            self.output_layer.weights -= output_layer_weights_step_size\n",
    "            self.output_layer.biases  -= output_layer_bias_step_size\n",
    "        \n",
    "            print(f\"New b_3: {self.output_layer.biases}\")\n",
    "            print(f\"New w3: {self.output_layer.weights[0, 0]}\")\n",
    "            print(f\"New w4: {self.output_layer.weights[1, 0]}\")\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf7eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [\n",
    "    DenseLayer(\n",
    "        input_size=1,\n",
    "        output_size=2,\n",
    "        weights=np.array([ \n",
    "            [3.34, -3.53]\n",
    "        ]),\n",
    "        biases=np.array(\n",
    "            [-1.43, 0.57]\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "output_layer: DenseLayer = DenseLayer(\n",
    "    input_size=2,\n",
    "    output_size=1,\n",
    "    weights=np.array([ \n",
    "        [1.0], [1.0] # default weights\n",
    "    ]),        \n",
    "    biases=np.array(\n",
    "        [0.0] # defautl bias\n",
    "    )\n",
    ")\n",
    "\n",
    "nn = NeuralNetwork_E(\n",
    "    input_size=1,\n",
    "    hidden_layers=hidden_layers,\n",
    "    output_layer=output_layer\n",
    ")\n",
    "\n",
    "x_ticks: np.ndarray = np.linspace(0, 1)\n",
    "y_pred: np.ndarray = nn.predict(x_ticks)\n",
    "\n",
    "plt.plot(x_ticks, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a002a1db",
   "metadata": {},
   "source": [
    "## Treinando a rede neural e plotando o resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN: np.ndarray = np.array([0.0, 0.5, 1.0])\n",
    "Y_TRAIN: np.ndarray = np.array([0.0, 1.0, 0.0])\n",
    "nn.fit(\n",
    "    x_train=X_TRAIN,\n",
    "    y_train=Y_TRAIN,\n",
    "    learning_rate=0.125,\n",
    "    epochs=500\n",
    ")\n",
    "\n",
    "x_ticks: np.ndarray = np.linspace(0, 1)\n",
    "y_pred: np.ndarray = nn.predict(x_ticks)\n",
    "\n",
    "plt.plot(x_ticks, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
