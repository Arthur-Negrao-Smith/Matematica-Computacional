{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d19e28",
   "metadata": {},
   "source": [
    "# Questão A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc6884",
   "metadata": {},
   "source": [
    "## Instalando as dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882f1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q matplotlib\n",
    "%pip install -q numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c016e3ee",
   "metadata": {},
   "source": [
    "## Importando o numpy e o matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7931d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829ca061",
   "metadata": {},
   "source": [
    "## Criando as funções básicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee6f0d",
   "metadata": {},
   "source": [
    "### Função linear para calcular os valores dos pontos\n",
    "\n",
    "`x` será o conjunto de pontos que deseja ser calculado. `x` será um numpy array para permitir um \n",
    "cálculo vetorial mais eficiente.\n",
    "\n",
    "A ideia de vetorizar se dá a partir dos seguintes passos:\n",
    "\n",
    "A função linear se dá pela fórmula $f(x) = ax + b$, substituindo o $f(x)$ por $y$, temos: $y = ax + b$. \n",
    "Faremos isso para todo o conjunto de pontos, então podemos escrever:\n",
    "\n",
    "$\\{y_1, y_2, ..., y_n\\} = \\{ax_1 + b, ax_2 + b, \\cdots, ax_n + b\\}$\n",
    "\n",
    "Agora podemos transformar o conjunto de $y$ como um vetor $\\vec{y}$ de n dimensões:\n",
    "\n",
    "$\\vec{y} = \\{ax_1 + b, ax_2 + b, \\cdots, ax_n + b\\}$\n",
    "\n",
    "Outra forma de visualização de $\\vec{y}$ é por meio matricial:\n",
    "\n",
    "$$\n",
    "\\vec{y} = \n",
    "\\begin{pmatrix}\n",
    "y_1    \\\\\n",
    "y_2    \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Podemos isolar o $b$ que se repete em todos as colunas como um vetor $\\vec{b}$ de $n$ dimensões:\n",
    "\n",
    "$\\vec{y} = \\{ax_1, ax_2, \\cdots, ax_n\\} + \\vec{b}$\n",
    "\n",
    "Ou, na forma matricial:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "y_1    \\\\\n",
    "y_2    \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{pmatrix} =\n",
    "\n",
    "\\begin{pmatrix}\n",
    "ax_1   \\\\\n",
    "ax_2   \\\\\n",
    "\\vdots \\\\\n",
    "ax_n\n",
    "\\end{pmatrix}\n",
    "\n",
    "+\n",
    "\n",
    "\\begin{pmatrix}\n",
    "b      \\\\\n",
    "b      \\\\\n",
    "\\vdots \\\\\n",
    "b\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Agora, transformamos \"$a$\" em um escalar por estar multiplicando igualmente todos os $x$ e o \n",
    "conjunto $\\{x_1, x_2, ..., x_n\\}$ \n",
    "como um vetor $\\vec{x}$. Assim a fórmula pode ser reescrita por:\n",
    "\n",
    "$\\vec{y} = a\\vec{x} + \\vec{b}$\n",
    "\n",
    "Ou na forma matricial:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "y_1    \\\\\n",
    "y_2    \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{pmatrix} = a \\cdot\n",
    "\n",
    "\\begin{pmatrix}\n",
    "x_1    \\\\\n",
    "x_2    \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{pmatrix}\n",
    "\n",
    "+\n",
    "\n",
    "\\begin{pmatrix}\n",
    "b      \\\\\n",
    "b      \\\\\n",
    "\\vdots \\\\\n",
    "b\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Logo, é possível utilizar um array como um vetor para o cálculo inteiro, sendo o $\\vec{x}$ o vetor de\n",
    "entrada, o '$a$' representa o \"slope\" e o $\\vec{b}$ representa o \"bias\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef407e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_yPoints_linear_function(x: np.ndarray, bias: float, slope: float) -> np.ndarray:\n",
    "    \"\"\"Calcule the current y from f(x) = weight * slope + bias\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Current x in X axle\n",
    "        bias (float): The 'b' in f(x) = ax + b\n",
    "        slope (float): The 'a' in f(x) = ax + b\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: y values from the linear function\n",
    "    \"\"\"\n",
    "    return bias + (slope * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d9bc1",
   "metadata": {},
   "source": [
    "### Fórmula do erro quadrado para identificar a distância do previsto para a resposta\n",
    "\n",
    "Essa função está encarregada de pegar os pontos que temos para treinar o modelo e utiliza\n",
    "para calcular os valores de que a regressão linear está prevendo, utilizamos cálculos vetoriais\n",
    "novamente para otimizar o código python. Essa utilização do vetor surge a partir dos seguintes\n",
    "passos:\n",
    "\n",
    "$SE = \\sum_{i=1}^{n} (y_i - y'_i)^2$\n",
    "\n",
    "O \"$SE$\" representa o erro quadrado (_square error_), o $y_i$ representa o conjunto $Y$ de pontos pra \n",
    "treino e $y'_i$ é o conjunto $Y$ previsto pela regressão linear utilizando os mesmos valores de $x$ que os \n",
    "valores reais. Assim, podemos quebrar o somatório em um cálculo vetorial por $Y$ ser representado \n",
    "como um vetor assim como visto anteriormente:\n",
    "\n",
    "$e = (\\vec{y} - \\vec{y'})$\n",
    "\n",
    "$SE = \\sum e^2$\n",
    "\n",
    "**Obs:** Para fins de esclarecimento, informo que a jogada algébrica utilizada foi apenas de considerar o somatório\n",
    "dos $y_i$ e os $y'_i$ como vetores $\\vec{y}$ e $\\vec{y'}$ sendo subtraídos:\n",
    "\n",
    "$$\n",
    "e =\n",
    "\n",
    "\\begin{pmatrix}\n",
    "y_1 - y'_1 \\\\\n",
    "y_2 - y'_2 \\\\\n",
    "\\vdots     \\\\\n",
    "y_n - y'_n\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "e o $e^2$ seria o quadrado de cada subtração:\n",
    "\n",
    "$$\n",
    "e^2 =\n",
    "\n",
    "\\begin{pmatrix}\n",
    "(y_1 - y'_1)^2 \\\\\n",
    "(y_2 - y'_2)^2 \\\\\n",
    "\\vdots         \\\\\n",
    "(y_n - y'_n)^2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "E o $\\sum e^2$ representa a soma de todos os quadrados da diferença do vetor equivalente ao `np.sum(error_array ** 2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6038300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_square_error(real_dots: np.ndarray, bias: float, slope: float) -> float:\n",
    "    \"\"\"Calcule the current square error\n",
    "\n",
    "    Args:\n",
    "        real_dots (np.ndarray): Real points (x, y) collected out of IA\n",
    "        bias (float): The 'b' in f(x) = ax + b\n",
    "        slope (float): The 'a' in f(x) = ax + b\n",
    "\n",
    "    Returns:\n",
    "        float: calcule from square error\n",
    "    \"\"\"\n",
    "\n",
    "    # x_real and y_real are real values collecteds\n",
    "    # calcule the error of the linear regression in x_real\n",
    "    x_real: np.ndarray = real_dots[:, 0]\n",
    "    y_real: np.ndarray = real_dots[:, 1]\n",
    "\n",
    "    # y_pred is the IA response\n",
    "    y_pred: np.ndarray = calcule_yPoints_linear_function(x=x_real, bias=bias, slope=slope)\n",
    "\n",
    "    error_array: np.ndarray = y_real - y_pred\n",
    "\n",
    "    # sum all squareerror array\n",
    "    return np.sum(error_array ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79669c6b",
   "metadata": {},
   "source": [
    "## Função para calcular a curva do erro quadrado para os valores $x$ possíveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d11754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_error_curve_values(x_ticks: np.ndarray, slope: float, real_dots: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Get all square error curve values in x linespace\n",
    "\n",
    "    Args:\n",
    "        x_ticks (np.ndarray): All x values to calculate (x axle)\n",
    "        slope (float): The 'a' in f(x) = ax + b (weight)\n",
    "        real_dots (np.ndarray): Real points (x, y) collected out of IA\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: All square error calculated\n",
    "    \"\"\"\n",
    "    \n",
    "    # x_ticks are all avaliable bias\n",
    "    # bias = intecept\n",
    "    y_errors_values: list[float] = [calcule_square_error(slope=slope, bias=bias, real_dots=real_dots) for bias in x_ticks]\n",
    "\n",
    "    return np.array(y_errors_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1396382",
   "metadata": {},
   "source": [
    "## Função para calcular a derivada parcial da função do erro quadrado\n",
    "\n",
    "Essa função busca utilizar o numpy reduzindo o overhead padrão do python, o cálculo\n",
    "da derivada parcial se dá através da regra da cadeia como será demostrado adiante.\n",
    "\n",
    "A função do erro quadrado é definida como:\n",
    "\n",
    "$SE = \\sum (\\vec{y} - \\vec{y'})^2$\n",
    "\n",
    "Porém, podemos representar a subtração vetorial como:\n",
    "\n",
    "$e = (\\vec{y} - \\vec{y'})$\n",
    "\n",
    "Agora podemos derivar o valor $SE$ em relação a $\\vec{y'}$: \n",
    "\n",
    "$\\frac{\\partial SE}{\\partial y'} = \\frac{\\partial SE}{\\partial \\vec{y'}}\\sum (\\vec{y} - \\vec{y'})^2$\n",
    "\n",
    "Podemos mudar utilizar \"$e$\" no lugar de $(\\vec{y} - \\vec{y'})$ e utilizar a regra da cadeia:\n",
    "\n",
    "$\\frac{\\partial SE}{\\partial y'} = \\sum \\frac{\\partial}{\\partial e}e^2 \\frac{\\partial}{\\partial \\vec{y'}}(\\vec{y} - \\vec{y'})$\n",
    "\n",
    "A derivada de $SE$ em relação a $e$:\n",
    "\n",
    "1. $\\frac{\\partial SE}{\\partial e} = \\frac{\\partial}{\\partial \\vec{y'}}e^2$\n",
    "2. $\\frac{\\partial SE}{\\partial e} = 2e$\n",
    "\n",
    "A derivada de $e$ em relação a $\\vec{y}$:\n",
    "\n",
    "1. $\\frac{\\partial e}{\\partial \\vec{y'}} = \\frac{\\partial}{\\partial \\vec{y'}}(\\vec{y} - \\vec{y'})$\n",
    "2. $\\frac{\\partial e}{\\partial \\vec{y'}} = -1$\n",
    "\n",
    "A derivada final:\n",
    "\n",
    "$\\frac{\\partial SE}{\\partial y'} = \\sum 2e \\times (-1)$\n",
    "\n",
    "Podemos novamente substituir o valor de \"$e$\" e passar as constantes multiplicativas para fora do somatório\n",
    "e obtemos o valor da derivada parcial:\n",
    "\n",
    "$\\frac{\\partial SE}{\\partial y'} = -2 \\sum (\\vec{y} - \\vec{y'})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf0697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_partial_derivative_bias(y_real: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Get the partial derivative of square error curve\n",
    "\n",
    "    Args:\n",
    "        y_real (np.ndarray): Real points collected\n",
    "        y_pred (np.ndarray): Linear Regression points (same lenght of points with `y_real`)\n",
    "\n",
    "    Returns:\n",
    "        float: Partial derivative\n",
    "    \"\"\"\n",
    "\n",
    "    # -2 * (real_value - (bias + slope * x) + ... # x is a independent variable\n",
    "    # this is the patial derivate\n",
    "    # y_pred is a array with all values predicted by the linear regression\n",
    "    # y_real is a arrrary with all real values\n",
    "    return -2 * np.sum(y_real - y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c383a1",
   "metadata": {},
   "source": [
    "## Função para calcular a tangente do erro quadrado\n",
    "\n",
    "Essa função usará a equação da reta para gerar uma reta tangente com a fim de\n",
    "demonstrar a derivada em relação àquele ponto. Como o \"$m$\" representa a inclinação\n",
    "da reta, então ele será equivalente a derivada parcial do _bias_.\n",
    "\n",
    "A equação da reta possui a seguinte fórmula: $y = m * (x - x_0) + y_0$\n",
    "\n",
    "Como o \"$y$\", o \"$y_0$\" o \"$x$\" e o \"$x_0$\" representam um conjunto de pontos, posso novamente representa-los\n",
    "como vetores, respectivamente: $\\vec{y}$, $\\vec{y_0}$, $\\vec{x}$ e $\\vec{x_0}$. Matricialmente essa fórmula\n",
    "fica:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "y_1    \\\\\n",
    "y_2    \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{pmatrix} = m \\cdot\n",
    "\n",
    "\\left[\n",
    "\\begin{pmatrix}\n",
    "x_1    \\\\\n",
    "x_2    \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{pmatrix} - \\vec{x_0}\n",
    "\\right]\n",
    "\n",
    "+ \\vec{y_0}\n",
    "$$\n",
    "\n",
    "Simplificando a notação $\\vec{x}-{x_0}$:\n",
    "\n",
    "$$\n",
    "\\vec{x} - \\vec{x_0} = \n",
    "\n",
    "\\begin{pmatrix}\n",
    "x_1 - x_0 \\\\\n",
    "x_2 - x_0 \\\\\n",
    "\\vdots    \\\\\n",
    "x_n - x_0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Agora podemos representar o cálculo como:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "y_1    \\\\\n",
    "y_2    \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{pmatrix} = m \\cdot\n",
    "\n",
    "\\begin{pmatrix}\n",
    "x_1 - x_0 \\\\\n",
    "x_2 - x_0 \\\\\n",
    "\\vdots    \\\\\n",
    "x_n - x_0\n",
    "\\end{pmatrix} +\n",
    "\n",
    "\\begin{pmatrix}\n",
    "y_0    \\\\\n",
    "y_0    \\\\\n",
    "\\vdots \\\\\n",
    "y_0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**Obs:** Lembrando que o $x_0$ e o $y_0$ representam, respectivamente, o $b$ (`bias`) e o erro quadrado (`square_error`) daquele ponto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f073c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcule_tan_line(square_error: float, y_real: np.ndarray, y_pred: np.ndarray, bias: float, tan_range: float = 1.5) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Calcule the current tangent line of the square error curve\n",
    "\n",
    "    Args:\n",
    "        square_error (float): Error of the linear regression\n",
    "        y_real (np.ndarray): Real y values\n",
    "        y_pred (np.ndarray): Prediced y values by linear regression\n",
    "        bias (float): X in square error curve or (bias of the linear regression)\n",
    "        tan_range (float, optional): Range to calculate the tangent (x size). Defaults to 1.5.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray]: Tuple with current x tangent values and y tangent values\n",
    "    \"\"\"\n",
    "\n",
    "    # bias is a x variable in square error curve\n",
    "    x_tan: np.ndarray = np.linspace(\n",
    "        start=bias - tan_range,\n",
    "        stop=bias + tan_range,\n",
    "        num=len(y_pred)\n",
    "    )\n",
    "\n",
    "    # equation of the straight line\n",
    "    # y = m * (x - x0) + y0\n",
    "    y_tan: np.ndarray = calcule_partial_derivative_bias(y_real, y_pred) * (x_tan - bias) + square_error\n",
    "    return (x_tan, y_tan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbbd1cb",
   "metadata": {},
   "source": [
    "## Funções para criar os gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933157c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.axes import Axes\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "def create_subplots() -> tuple[Figure, tuple[Axes, Axes]]:\n",
    "    \"\"\"Create subplots to show animations\n",
    "\n",
    "    Returns:\n",
    "        tuple[Figure, tuple[Axes, ...]]: The figure and a tuple with all lines\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "    return fig, axes\n",
    "\n",
    "def config_axels(\n",
    "        axes: tuple[Axes, Axes], \n",
    "        real_dots: np.ndarray,\n",
    "        bias: float,\n",
    "        slope: float,\n",
    "        x_ticks_linear_regression: np.ndarray,\n",
    "        ypoints_linear_regression: np.ndarray,\n",
    "        x_ticks_error_curve: np.ndarray,\n",
    "        ypoints_square_error_curve: np.ndarray,\n",
    "        ) -> tuple:\n",
    "\n",
    "    # linear regrassion\n",
    "    ax1: Axes = axes[0]\n",
    "\n",
    "    # square error\n",
    "    ax2: Axes = axes[1]\n",
    "\n",
    "    linear_regression, = ax1.plot(x_ticks_linear_regression, ypoints_linear_regression)\n",
    "\n",
    "    # add individual real points\n",
    "    ax1.scatter(x=real_dots[:, 0], y=real_dots[:, 1], label='Pontos Reais', color='green', s=15, marker=\"o\")\n",
    "\n",
    "    ax1.set_title(\"Linear Regression\")\n",
    "    ax1.set_xlabel(\"weight\")\n",
    "    ax1.set_ylabel(\"height\")\n",
    "\n",
    "    # define origin to (0, 0)\n",
    "    ax1.set_xlim(left=0, right=x_ticks_linear_regression[-1])\n",
    "    ax1.set_ylim(bottom=0)\n",
    "\n",
    "    ax1.grid(visible=True, axis='both', alpha=0.5, color='gray')\n",
    "    bias_text = ax1.text(\n",
    "        0.1, # 10% x\n",
    "        0.9, # 90% y\n",
    "        f'Bias (intercept): {bias:.4f}',\n",
    "        transform=ax1.transAxes, \n",
    "        fontsize=12, \n",
    "        bbox=dict(facecolor='white', alpha=0.7)\n",
    "    )\n",
    "\n",
    "    ax2.plot(x_ticks_error_curve, ypoints_square_error_curve, label='Square Error')\n",
    "    ax2.set_title(\"Square Error\")\n",
    "    ax2.set_xlabel(\"Intercept (bias)\")\n",
    "    ax2.set_ylabel(\"Sum of Square Errors\")\n",
    "\n",
    "    square_error: float = calcule_square_error(real_dots, bias, 0.64)\n",
    "    x_real =  real_dots[:, 0]\n",
    "    y_real = real_dots[:, 1]\n",
    "    square_error_curve_tangent, = ax2.plot(\n",
    "        *calcule_tan_line(\n",
    "            square_error,\n",
    "            y_real,\n",
    "            calcule_yPoints_linear_function(x_real, bias, slope),\n",
    "            bias\n",
    "        ),\n",
    "        label='Derivative'\n",
    "        )\n",
    "    \n",
    "    ax2.grid(visible=True, axis='both', alpha=0.5, color='gray')\n",
    "    error_text = ax2.text(\n",
    "        0.1,\n",
    "        0.9,\n",
    "        f'Square Error: {square_error:.4f}',\n",
    "        transform=ax2.transAxes,\n",
    "        fontsize=12,\n",
    "        bbox=dict(facecolor='white', alpha=0.7)\n",
    "    )\n",
    "    \n",
    "    current_error_point, = ax2.plot(\n",
    "        [bias],\n",
    "        [square_error],\n",
    "        'ro',\n",
    "        markersize=8,\n",
    "        label='Current Error'\n",
    "    )\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    # define origin to (0, 0)\n",
    "    ax2.set_xlim(left=0, right=x_ticks_error_curve[-1])\n",
    "    ax2.set_ylim(bottom=0)\n",
    "\n",
    "    return linear_regression, square_error_curve_tangent, current_error_point, bias_text, error_text\n",
    "\n",
    "def gradient_descent_array(\n",
    "        initial_bias: float,\n",
    "        slope: float,\n",
    "        real_dots: np.ndarray,\n",
    "        learn_rate: float,\n",
    "        max_iterations: int,\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"Create a gradient descent array to update the linear regression bias\n",
    "\n",
    "    Args:\n",
    "        initial_bias (float): First bias of the linear regression\n",
    "        slope (float): The 'a' of linear equation: f(x) = ax + b\n",
    "        real_dots (np.ndarray): Real dots of data\n",
    "        learn_rate (float): Multiply factor of the de step size\n",
    "        max_iterations (int): Limit of iterations\n",
    "\n",
    "    Yields:\n",
    "        np.ndarray[float]: Array with updated bias\n",
    "    \"\"\"\n",
    "\n",
    "    bias_list: list[float] = []\n",
    "    current_bias: float = initial_bias\n",
    "    for _ in range(max_iterations):\n",
    "        x_real: np.ndarray = real_dots[:, 0]\n",
    "        y_real: np.ndarray = real_dots[:, 1]\n",
    "        y_pred: np.ndarray = calcule_yPoints_linear_function(\n",
    "            x=x_real,\n",
    "            bias=current_bias,\n",
    "            slope=slope,\n",
    "        )\n",
    "\n",
    "        partial_derivative: float = calcule_partial_derivative_bias(\n",
    "            y_real=y_real,\n",
    "            y_pred=y_pred,\n",
    "        )\n",
    "\n",
    "        step_size: float = partial_derivative * learn_rate\n",
    "\n",
    "        current_bias -= step_size\n",
    "       \n",
    "        if step_size == 0:\n",
    "            break\n",
    "\n",
    "        bias_list.append(current_bias)\n",
    "    \n",
    "    return np.array(bias_list)\n",
    "        \n",
    "\n",
    "def animate( \n",
    "        current_bias: float,\n",
    "        x_ticks_linear_regression: np.ndarray,\n",
    "        real_dots: np.ndarray,\n",
    "        lines: tuple[Line2D, ...],\n",
    "        slope: float = 0.0,\n",
    "        tan_range: float = 1.5,\n",
    "        ) -> tuple[Line2D, ...]:\n",
    "    \"\"\"Animate the new values of the linear regression and the square error function\n",
    "\n",
    "    Args:\n",
    "        current_bias (float): Current 'b' (intercept) of the linear regression: f(x) = ax + b\n",
    "        x_ticks_linear_regression (np.ndarray): X values to calculate the linear regression\n",
    "        real_dots (np.ndarray): Real dots to use to adapt the linear regression\n",
    "        lines (tuple[Line2D, ...]): Lines with graphics\n",
    "        slope (float, optional): The 'a' (weight) of the linear regression: f(x) = ax + b. Defaults to 0.0.\n",
    "        tan_range (float, optional): Range of the tangent line. Defaults to 1.5.\n",
    "\n",
    "    Returns:\n",
    "        tuple[Line2D, ...]: All lines to update graphics\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_regression, square_error_curve_tan, current_error_point, bias_text, error_text = lines\n",
    "\n",
    "    y_points_linear_regression: np.ndarray = calcule_yPoints_linear_function(\n",
    "        x=x_ticks_linear_regression,\n",
    "        bias=current_bias,\n",
    "        slope=slope\n",
    "    )\n",
    "    linear_regression.set_data(x_ticks_linear_regression, y_points_linear_regression)\n",
    "    bias_text.set_text(f'Bias (intercept): {current_bias:.4f}')\n",
    "\n",
    "    y_real: np.ndarray = real_dots[:, 1]\n",
    "    x_real: np.ndarray = real_dots[:, 0]\n",
    "    y_pred: np.ndarray = calcule_yPoints_linear_function(\n",
    "        x=x_real, \n",
    "        bias=current_bias, \n",
    "        slope=slope\n",
    "    )\n",
    "\n",
    "    square_error: float = calcule_square_error(real_dots, current_bias, slope)\n",
    "    x_tan, y_tan = calcule_tan_line(\n",
    "        square_error=square_error,\n",
    "        y_real=y_real,\n",
    "        y_pred=y_pred,\n",
    "        bias=current_bias,\n",
    "        tan_range=tan_range\n",
    "    )\n",
    "    square_error_curve_tan.set_data(x_tan, y_tan)\n",
    "\n",
    "    # actual curve point\n",
    "    current_error_point.set_data([current_bias], [square_error])\n",
    "\n",
    "    error_text.set_text(f'Square Error: {square_error:.4f}')\n",
    "\n",
    "    return linear_regression, square_error_curve_tan, current_error_point, bias_text, error_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7832dd00",
   "metadata": {},
   "source": [
    "## Constantes para controlar a regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee5fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TICKS_LINEAR_REGRESSION: np.ndarray = np.linspace(0, 3.5)\n",
    "X_TICKS_ERROR_CURVE: np.ndarray = np.linspace(0, 2)\n",
    "REAL_DOTS: np.ndarray = np.array([[0.5, 1.4], [2.3, 1.9], [2.9, 3.2]])\n",
    "INITIAL_BIAS: float = 0.0\n",
    "SLOPE = 0.64\n",
    "LEARN_RATE: float = 0.01\n",
    "MAX_ITERATIONS: int = 150\n",
    "TAN_RANGE: float = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa52ffc",
   "metadata": {},
   "source": [
    "## Pegando os valores iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc788f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_points_linear_regression: np.ndarray = calcule_yPoints_linear_function(\n",
    "    x=X_TICKS_LINEAR_REGRESSION,\n",
    "    bias=INITIAL_BIAS,\n",
    "    slope=SLOPE,\n",
    ")\n",
    "\n",
    "y_points_square_error: np.ndarray = calcule_error_curve_values(\n",
    "    x_ticks=X_TICKS_ERROR_CURVE,\n",
    "    slope=SLOPE,\n",
    "    real_dots=REAL_DOTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de543dcb",
   "metadata": {},
   "source": [
    "## Criando a figura e os eixos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a50be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = create_subplots()\n",
    "lines_to_update = config_axels(\n",
    "    axes=axes,\n",
    "    bias=INITIAL_BIAS,\n",
    "    slope=SLOPE,\n",
    "    real_dots=REAL_DOTS,\n",
    "    x_ticks_linear_regression=X_TICKS_LINEAR_REGRESSION,\n",
    "    ypoints_linear_regression=y_points_linear_regression,\n",
    "    x_ticks_error_curve=X_TICKS_ERROR_CURVE,\n",
    "    ypoints_square_error_curve=y_points_square_error,\n",
    ")\n",
    "\n",
    "bias_frame_array: np.ndarray = gradient_descent_array(\n",
    "    initial_bias=INITIAL_BIAS,\n",
    "    slope=SLOPE,\n",
    "    real_dots=REAL_DOTS,\n",
    "    learn_rate=LEARN_RATE,\n",
    "    max_iterations=MAX_ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93f85c0",
   "metadata": {},
   "source": [
    "## Iniciando a animação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083cbff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "\n",
    "ani = FuncAnimation(\n",
    "    fig=fig,\n",
    "    func=animate,\n",
    "    frames=bias_frame_array,\n",
    "    fargs=(\n",
    "        X_TICKS_LINEAR_REGRESSION,\n",
    "        REAL_DOTS,\n",
    "        lines_to_update,\n",
    "        SLOPE,\n",
    "        TAN_RANGE,\n",
    "    ),\n",
    "    interval=100, # Interval between frames\n",
    "    blit=True,    # Otmization: just draw the changes\n",
    "    repeat=False  # Do not reapeat the animation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8104d8c",
   "metadata": {},
   "source": [
    "## Mostrando a animação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3599346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "plt.close(fig)\n",
    "HTML(ani.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
